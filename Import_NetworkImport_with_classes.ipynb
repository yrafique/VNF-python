{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "#Parameters:\n",
    "NoOfUsers = 30\n",
    "cloud_server_cost = 1000 #($/per server/hour)\n",
    "edge_server_cost = 100   #($/per server/hour)\n",
    "propagation_delay_constant = 1.5  # in ms/km\n",
    "routing_cost = 0.09 #($/GB)\n",
    "device_path = 'Devices/EdgeDevices.json'\n",
    "containers_json_path = 'Devices/containers.json'\n",
    "iot_device_json = 'Devices/Traffic.json'\n",
    "networkname = 'Networks/newyork.xml'  # Update this to the correct path to your XML file\n",
    "\n",
    "# Set up logging\n",
    "DEBUG_MODE = True  # Change this to False to turn off debug logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='network_debug.log', level=logging.DEBUG, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "def debug_print(message, debug):\n",
    "    \"\"\"Prints message if debug is True and logs the message.\"\"\"\n",
    "    if debug:\n",
    "        print(message)\n",
    "    logging.debug(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fce9a7474fc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Populate the network graph with nodes, links, and specifying '5g' as the default medium\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mpopulate_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_medium\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'5g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDEBUG_MODE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdemands\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_XMLnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetworkname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDEBUG_MODE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nodes' is not defined"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from network_classes import *  # Import all network-related classes\n",
    "from network_functions import *  # Import all network-related functions\n",
    "from Import_NetworkFromXML import *\n",
    "\n",
    "# Set global debug mode\n",
    "DEBUG_MODE = True  # Change this to False to disable debug outputs\n",
    "\n",
    "\n",
    "# Initialize a new NetworkX graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Populate the network graph with nodes, links, and specifying '5g' as the default medium\n",
    "populate_network(G, nodes, links, default_medium='5g', debug=DEBUG_MODE)\n",
    "\n",
    "nodes, links, demands = read_XMLnetwork(networkname, debug=DEBUG_MODE)\n",
    "populate_network(G, networkname, default_medium='5g', debug=DEBUG_MODE)\n",
    "# and so on for other function calls\n",
    "\n",
    "\n",
    "# Find and set the MEC server based on edge servers' locations\n",
    "edge_servers = find_edge_servers(G, debug=DEBUG_MODE)\n",
    "edge_mid_point = set_mec_server(G, edge_servers, debug=DEBUG_MODE)\n",
    "\n",
    "# Report network health and measure network stats\n",
    "report_network_health(G, debug=DEBUG_MODE)\n",
    "measure_network_stats(G, debug=DEBUG_MODE)\n",
    "\n",
    "# Draw the network graph, highlighting the MEC server\n",
    "draw_network_graph(G, mec_location='mec_server', debug=DEBUG_MODE)\n",
    "\n",
    "# Print attributes for nodes and links to understand their details\n",
    "print_node_attributes(G, debug=DEBUG_MODE)\n",
    "print_link_attributes(G, debug=DEBUG_MODE)\n",
    "\n",
    "# Number of Users - replace 'NoOfUsers' with actual number, for example, 100\n",
    "NoOfUsers = 100  # Replace this with your actual number of users\n",
    "\n",
    "# Create the users and their connections\n",
    "G_users, users, user_connections = create_users(G, NoOfUsers, debug=DEBUG_MODE)\n",
    "print_user_details(users, debug=DEBUG_MODE)  # Call the print function to display user details\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Users\n",
    "Here we will generate users\n",
    "\n",
    "Generate Users: Randomly distrubuted users generated with uniform random distrubution of (x,y) corrdinates around the graph G that represents infrastrcuture. \n",
    "\n",
    "Allow options (ex. User_Spread)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'User' object has no attribute 'associated_ap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-effb69341e33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the users and their connections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mG_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_connections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoOfUsers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint_user_details\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Call the print function to display user details\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdraw_network_and_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_connections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\youse\\OneDrive\\Documents\\GitHub\\VNF-python\\network_functions.py\u001b[0m in \u001b[0;36mprint_user_details\u001b[1;34m(users)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[0muser_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfield_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"User ID\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Position\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Associated AP\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[1;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0muser_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massociated_ap\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'User' object has no attribute 'associated_ap'"
     ]
    }
   ],
   "source": [
    "# Create the users and their connections\n",
    "G_users, users, user_connections = create_users(G, NoOfUsers)\n",
    "print_user_details(users)  # Call the print function to display user details\n",
    "draw_network_and_users(G, G_users, user_connections)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Server Properties to each node in a uniform manner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------------------------------+---------------------------------+------------+--------------+---------+--------+---------+---------------------------------+-------------+\n",
      "|     Node    |                    pos                    |               name              | formFactor | architecture |   cpu   | memory | storage |              source             | server_cost |\n",
      "+-------------+-------------------------------------------+---------------------------------+------------+--------------+---------+--------+---------+---------------------------------+-------------+\n",
      "|      N1     |               (237.0, 338.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|      N2     |               (130.0, 207.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|      N3     |               (126.0, 531.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|      N4     |               (209.0, 523.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|      N5     |               (345.0, 232.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|      N6     |               (95.0, 350.0)               |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|      N7     |               (302.0, 404.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|      N8     |               (161.0, 454.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|      N9     |               (394.0, 499.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|     N10     |               (389.0, 656.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|     N11     |               (237.0, 645.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|     N12     |               (277.0, 70.0)               |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|     N13     |               (540.0, 147.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|     N14     |               (592.0, 503.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|     N15     |               (490.0, 349.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|     N16     |               (463.0, 543.0)              |  Cisco HyperFlex HX220c Edge M5 |   16408    |     x86      | 34.8GHz |  1TB   |   5TB   |     Edge_Device_Cisco_HX220     |     100     |\n",
      "|  OhioCloud  |  (-400.1383141847919, -830.0059209290719) | Huawei FusionServer Pro 2298 V5 |   34253    |     x86      | 75.6GHz |  3TB   |  450TB  | Edge_Device_FusionServerPro2298 |     1000    |\n",
      "| OregonCloud | (-430.8275515896039, -120.55721609633432) | Huawei FusionServer Pro 2298 V5 |   34253    |     x86      | 75.6GHz |  3TB   |  450TB  | Edge_Device_FusionServerPro2298 |     1000    |\n",
      "+-------------+-------------------------------------------+---------------------------------+------------+--------------+---------+--------+---------+---------------------------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "# Call the function with the JSON file path\n",
    "associate_edge_devices(G, device_path, cloud_server_cost, edge_server_cost)    \n",
    "print_edge_device_attributes(G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Applications to users randomly to generate demand:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 30\n",
      "+---------+---------------------+------------------+--------------+------------------------+------------------+\n",
      "|   User  |     Application     | Bandwidth (Mbps) | Latency (ms) | Device Density (/km^2) |      Source      |\n",
      "+---------+---------------------+------------------+--------------+------------------------+------------------+\n",
      "|  user_0 |     Home Energy     |      0.0505      |    250.0     |          6000          |      Gungor      |\n",
      "|  user_1 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "|  user_2 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "|  user_3 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "|  user_4 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "|  user_5 |  Autonomous Traffic |      5.025       |      10      |         12000          | DeutscheTelekom  |\n",
      "|  user_6 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "|  user_7 |  Autonomous Traffic |      5.025       |      10      |         12000          | DeutscheTelekom  |\n",
      "|  user_8 |  Autonomous Traffic |      5.025       |      10      |         12000          | DeutscheTelekom  |\n",
      "|  user_9 |     Home Energy     |      0.0505      |    250.0     |          6000          |      Gungor      |\n",
      "| user_10 |  Structural Health  |       75.0       |     10.5     |         60000          |    Marabissi     |\n",
      "| user_11 | Connected Ambulance |      1000.0      |      10      |           60           |   Cisotto2019    |\n",
      "| user_12 |     Road Safety     |      0.005       |     55.0     |          3000          |      Schulz      |\n",
      "| user_13 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "| user_14 |     Home Energy     |      0.0505      |    250.0     |          6000          |      Gungor      |\n",
      "| user_15 | Connected Ambulance |      1000.0      |      10      |           60           |   Cisotto2019    |\n",
      "| user_16 |     Home Energy     |      0.0505      |    250.0     |          6000          |      Gungor      |\n",
      "| user_17 |     Road Safety     |      0.005       |     55.0     |          3000          |      Schulz      |\n",
      "| user_18 |     Road Safety     |      0.005       |     55.0     |          3000          |      Schulz      |\n",
      "| user_19 | Connected Ambulance |      1000.0      |      10      |           60           |   Cisotto2019    |\n",
      "| user_20 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "| user_21 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "| user_22 | Connected Ambulance |      1000.0      |      10      |           60           |   Cisotto2019    |\n",
      "| user_23 |     Road Safety     |      0.005       |     55.0     |          3000          |      Schulz      |\n",
      "| user_24 |  Remote Monitoring  |       5.0        |     250      |         60000          |   Cisotto2019    |\n",
      "| user_25 |  Structural Health  |       75.0       |     10.5     |         60000          |    Marabissi     |\n",
      "| user_26 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "| user_27 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "| user_28 |     Home Energy     |      0.0505      |    250.0     |          6000          |      Gungor      |\n",
      "| user_29 |  Structural Health  |       75.0       |     10.5     |         60000          |    Marabissi     |\n",
      "+---------+---------------------+------------------+--------------+------------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "associate_user_devices(G_users, iot_device_json)\n",
    "print(f\"Total users: {len(G_users.nodes())}\")\n",
    "print_user_device_attributes(G_users)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign each application and its users to Containers (network_functions + analytics_functions) following order defined in sfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------------------------------------------------------------+---------------------------------------------------+---------------+-----------------+-------------------+--------------------+----------------+-----------------+-------------------+--------------------+------------------------+-----------------+-------------------+--------------------+---------------------------+-----------------+-------------------+--------------------+\n",
      "|     Application     |                         Service Function Chain                         |                  Users Associated                 |   CNF 1 Name  | CNF 1 CPU (GHz) | CNF 1 Memory (GB) | CNF 1 Storage (GB) |   CNF 2 Name   | CNF 2 CPU (GHz) | CNF 2 Memory (GB) | CNF 2 Storage (GB) |       CNF 3 Name       | CNF 3 CPU (GHz) | CNF 3 Memory (GB) | CNF 3 Storage (GB) |         CNF 4 Name        | CNF 4 CPU (GHz) | CNF 4 Memory (GB) | CNF 4 Storage (GB) |\n",
      "+---------------------+------------------------------------------------------------------------+---------------------------------------------------+---------------+-----------------+-------------------+--------------------+----------------+-----------------+-------------------+--------------------+------------------------+-----------------+-------------------+--------------------+---------------------------+-----------------+-------------------+--------------------+\n",
      "|     Road Safety     |        Firewall -> VPN -> Safety Protocol -> Alert Notification        |         user_12, user_17, user_18, user_23        |    Firewall   |        2        |         4         |         5          |      VPN       |        1        |         2         |         2          |    Safety Protocol     |        1        |         1         |         1          |     Alert Notification    |       0.5       |        0.5        |        0.5         |\n",
      "|  City Surveillance  | Proxy -> Data Storage -> Face Recognition -> License Plate Recognition |          user_1, user_2, user_13, user_26         |     Proxy     |        4        |         16        |         20         |  Data Storage  |        2        |         8         |         50         |    Face Recognition    |        3        |         8         |         10         | License Plate Recognition |        2        |         4         |         5          |\n",
      "|  Structural Health  |    Cache -> Data Analysis -> Vibration Analysis -> Health Reporting    |             user_10, user_25, user_29             |     Cache     |        1        |         2         |         5          | Data Analysis  |        2        |         4         |         10         |   Vibration Analysis   |        1        |         1         |         2          |      Health Reporting     |        1        |         1         |         1          |\n",
      "|     Home Energy     |  Energy Router -> Energy Storage -> HVAC Control -> Lighting Control   |     user_0, user_9, user_14, user_16, user_28     | Energy Router |        1        |         1         |         1          | Energy Storage |        1        |         2         |         2          |      HVAC Control      |        1        |         1         |         1          |      Lighting Control     |       0.5       |        0.5        |        0.5         |\n",
      "|     Smart Grids     |     Load Balancer -> CDN -> Energy Forecasting -> Fault Detection      | user_3, user_4, user_6, user_20, user_21, user_27 | Load Balancer |        1        |         2         |         2          |      CDN       |        1        |         1         |         1          |   Energy Forecasting   |        1        |         1         |         1          |      Fault Detection      |       0.5       |        0.5        |        0.5         |\n",
      "| Connected Ambulance |    Proxy -> IDS/IPS -> Vital Signs Monitoring -> Communication Link    |         user_11, user_15, user_19, user_22        |     Proxy     |        2        |         4         |         5          |    IDS/IPS     |        1        |         2         |         2          | Vital Signs Monitoring |        1        |         1         |         1          |     Communication Link    |        1        |         1         |         1          |\n",
      "|  Remote Monitoring  |          Router -> Firewall -> Alert System -> Remote Control          |                      user_24                      |     Router    |        1        |         2         |         3          |    Firewall    |        2        |         4         |         5          |      Alert System      |        1        |         1         |         1          |       Remote Control      |        1        |         1         |         1          |\n",
      "+---------------------+------------------------------------------------------------------------+---------------------------------------------------+---------------+-----------------+-------------------+--------------------+----------------+-----------------+-------------------+--------------------+------------------------+-----------------+-------------------+--------------------+---------------------------+-----------------+-------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "\n",
    "app_to_users, app_objects, container_data = associate_app_with_containers(G_users, containers_json_path)\n",
    "# Assuming containers_data is already loaded\n",
    "#print_container_data(container_data)\n",
    "\n",
    "print_all_applications_and_resources(app_objects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Microservice Placement + Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 30\n",
      "+---------+---------------------+------------------+--------------+------------------------+------------------+\n",
      "|   User  |     Application     | Bandwidth (Mbps) | Latency (ms) | Device Density (/km^2) |      Source      |\n",
      "+---------+---------------------+------------------+--------------+------------------------+------------------+\n",
      "|  user_0 | Connected Ambulance |      1000.0      |      10      |           60           |   Cisotto2019    |\n",
      "|  user_1 |     Home Energy     |      0.0505      |    250.0     |          6000          |      Gungor      |\n",
      "|  user_2 |  Autonomous Traffic |      5.025       |      10      |         12000          | DeutscheTelekom  |\n",
      "|  user_3 |  Structural Health  |       75.0       |     10.5     |         60000          |    Marabissi     |\n",
      "|  user_4 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "|  user_5 |  Remote Monitoring  |       5.0        |     250      |         60000          |   Cisotto2019    |\n",
      "|  user_6 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "|  user_7 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "|  user_8 |     Road Safety     |      0.005       |     55.0     |          3000          |      Schulz      |\n",
      "|  user_9 |     Road Safety     |      0.005       |     55.0     |          3000          |      Schulz      |\n",
      "| user_10 |     Home Energy     |      0.0505      |    250.0     |          6000          |      Gungor      |\n",
      "| user_11 |  Autonomous Traffic |      5.025       |      10      |         12000          | DeutscheTelekom  |\n",
      "| user_12 |     Road Safety     |      0.005       |     55.0     |          3000          |      Schulz      |\n",
      "| user_13 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "| user_14 |  Remote Monitoring  |       5.0        |     250      |         60000          |   Cisotto2019    |\n",
      "| user_15 |  Structural Health  |       75.0       |     10.5     |         60000          |    Marabissi     |\n",
      "| user_16 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "| user_17 |     Home Energy     |      0.0505      |    250.0     |          6000          |      Gungor      |\n",
      "| user_18 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "| user_19 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "| user_20 |     Home Energy     |      0.0505      |    250.0     |          6000          |      Gungor      |\n",
      "| user_21 |     Smart Grids     |      0.7505      |     10.5     |          6000          |      Schulz      |\n",
      "| user_22 |  Structural Health  |       75.0       |     10.5     |         60000          |    Marabissi     |\n",
      "| user_23 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "| user_24 |  Autonomous Traffic |      5.025       |      10      |         12000          | DeutscheTelekom  |\n",
      "| user_25 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "| user_26 |  Remote Monitoring  |       5.0        |     250      |         60000          |   Cisotto2019    |\n",
      "| user_27 |  Remote Monitoring  |       5.0        |     250      |         60000          |   Cisotto2019    |\n",
      "| user_28 |  City Surveillance  |       60.0       |      10      |           60           | Munoz, Marabissi |\n",
      "| user_29 |  Structural Health  |       75.0       |     10.5     |         60000          |    Marabissi     |\n",
      "+---------+---------------------+------------------+--------------+------------------------+------------------+\n",
      "['user_0', 'user_1', 'user_2', 'user_3', 'user_4', 'user_5', 'user_6', 'user_7', 'user_8', 'user_9', 'user_10', 'user_11', 'user_12', 'user_13', 'user_14', 'user_15', 'user_16', 'user_17', 'user_18', 'user_19', 'user_20', 'user_21', 'user_22', 'user_23', 'user_24', 'user_25', 'user_26', 'user_27', 'user_28', 'user_29']\n",
      "{'pos': (242.83683286502196, 293.2003847541284), 'associated_ap': 'N1', 'user': <network_classes.User object at 0x00000221D1E866D8>}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'measured_bwd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5bd3fddb9389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0maccepted_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrejected_users\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_routing_for_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_to_node_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink_utilization_random\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_routing_cost_random\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouting_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpropagation_delays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0maccepted_users_count_random\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccepted_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\youse\\OneDrive\\Documents\\GitHub\\VNF-python\\network_functions.py\u001b[0m in \u001b[0;36mprocess_routing_for_users\u001b[1;34m(users, user_to_node_map, server_node_id, G, G_users, link_utilization, total_routing_cost, routing_cost, propagation_delays)\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_users\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_users\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m         \u001b[0mbandwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG_users\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'measured_bwd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    657\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_feasible_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_node_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink_utilization\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'measured_bwd'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import networkx as nx\n",
    "from network_functions import *  # This imports all your predefined functions\n",
    "\n",
    "# Initialize data structures to hold server and link states\n",
    "server_utilization_random = defaultdict(lambda: {'cpu': 0.0, 'memory': 0.0, 'storage': 0.0})\n",
    "link_utilization_random = defaultdict(float)\n",
    "accepted_users_count_random = 0\n",
    "rejected_users_count_random = 0\n",
    "total_routing_cost_random = 0.0\n",
    "total_propagation_delay_random = 0.0\n",
    "total_cloud_servers_used_random = 0\n",
    "total_edge_servers_used_random = 0\n",
    "\n",
    "associate_user_devices(G_users, iot_device_json)\n",
    "print(f\"Total users: {len(G_users.nodes())}\")\n",
    "print_user_device_attributes(G_users)\n",
    "\n",
    "# Assuming app_to_users, G, and G_users are already defined\n",
    "# Assuming containers_data is already loaded\n",
    "\n",
    "# Create users and associate them with their nearest access point\n",
    "G_users, users, user_connections = create_users(G, NoOfUsers, node_range=50)  # Adjust NoOfUsers and node_range as needed\n",
    "\n",
    "# Map users to their associated nodes (assuming 'associated_ap' is the attribute that contains this information)\n",
    "user_to_node_map = {user.id: user.associated_ap for user in users}\n",
    "\n",
    "# Extract propagation delays from the graph\n",
    "propagation_delays = {(u, v): data['latency'] for u, v, data in G.edges(data=True)}\n",
    "\n",
    "# Main logic for processing applications and users\n",
    "for app_name, users in app_to_users.items():\n",
    "    server = random.choice(list(G.nodes()))\n",
    "    total_cpu, total_mem, total_storage = gather_total_resources(app_name, container_data)\n",
    "\n",
    "    if check_and_update_server_utilization(server, total_cpu, total_mem, total_storage, server_utilization_random, G):\n",
    "        if \"cloud\" in server.lower():\n",
    "            total_cloud_servers_used_random += 1\n",
    "        else:\n",
    "            total_edge_servers_used_random += 1\n",
    "    else:\n",
    "        print(f\"Server capacity exceeded for server {server}\")\n",
    "        continue\n",
    "\n",
    "    accepted_users, rejected_users = process_routing_for_users(users, user_to_node_map, server, G, G_users, link_utilization_random, total_routing_cost_random, routing_cost, propagation_delays)\n",
    "\n",
    "    accepted_users_count_random += len(accepted_users)\n",
    "    rejected_users_count_random += len(rejected_users)\n",
    "\n",
    "# Print results and statistics\n",
    "print(f\"\\nTotal Routing Cost: {total_routing_cost_random}\")\n",
    "print(f\"\\nTotal Accepted Users: {accepted_users_count_random}\")\n",
    "print(f\"\\nPercentage of Accepted Users: {accepted_users_count_random / len(G_users.nodes()) * 100}%\")\n",
    "print(f\"\\nTotal Rejected Users: {rejected_users_count_random}\")\n",
    "print(f\"\\nPercentage of Rejected Users: {rejected_users_count_random / len(G_users.nodes()) * 100}%\")\n",
    "print(f\"\\nTotal Propagation Delay: {total_propagation_delay_random} ms\")\n",
    "print(f\"\\nTotal Edge Servers Used: {total_edge_servers_used_random}\")\n",
    "print(f\"\\nTotal Cloud Servers Used: {total_cloud_servers_used_random}\")\n",
    "total_cost_random = total_routing_cost_random + total_edge_servers_used_random * edge_server_cost + total_cloud_servers_used_random * cloud_server_cost\n",
    "print(f\"\\nTotal cost: {total_cost_random}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2f5054f7c9c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# Get the total resource requirements for this application\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mcontainer_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontainers_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Containers'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'application'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mapp_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontainer_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mcontainers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontainer_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize data structures to hold server and link states\n",
    "server_utilization_random = defaultdict(lambda: {'cpu': 0.0, 'memory': 0.0, 'storage': 0.0})\n",
    "link_utilization_random = defaultdict(float)\n",
    "accepted_users_count_random = 0\n",
    "rejected_users_count_random = 0\n",
    "total_routing_cost_random = 0.0\n",
    "link_cost_random = defaultdict(float)  # New dictionary to store the cost of each link\n",
    "total_propagation_delay_random = 0.0  # Initialize variable to hold total propagation delay\n",
    "# Initialize additional counters\n",
    "total_edge_servers_used_random = 0\n",
    "total_cloud_servers_used_random = 0\n",
    "total_cost_random = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to convert string capacity to float (e.g., '3TB' to 3000.0)\n",
    "def convert_capacity_to_float(capacity_str):\n",
    "    if 'GHz' in capacity_str:\n",
    "        return float(capacity_str.replace('GHz', ''))\n",
    "    elif 'x' in capacity_str:\n",
    "        # Assuming the format is 'cores x speed' and you want total GHz\n",
    "        cores, speed = capacity_str.split('x')\n",
    "        return float(cores) * float(speed)\n",
    "    elif 'TB' in capacity_str:\n",
    "        return float(capacity_str.replace('TB', '')) * 1000  # Convert TB to GB\n",
    "    else:\n",
    "        return float(capacity_str)\n",
    "\n",
    "\n",
    "# Function to find a path that can handle the given bandwidth\n",
    "def find_feasible_path(G, source, target, bandwidth):\n",
    "    for path in nx.all_simple_paths(G, source=source, target=target, cutoff=10):  # cutoff is optional, for performance\n",
    "        can_use_path = True\n",
    "        for i in range(len(path) - 1):\n",
    "            link = (path[i], path[i + 1])\n",
    "            link_capacity = G[path[i]][path[i + 1]]['capacity']  # Assuming capacity is in Mbps\n",
    "            if link_utilization_random[link] + bandwidth > link_capacity:\n",
    "                can_use_path = False\n",
    "                break\n",
    "        if can_use_path:\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "\n",
    "accepted_users_count_random = 0\n",
    "rejected_users_count_random = 0\n",
    "\n",
    "# Random Microservice Placement\n",
    "for app_name, users in app_to_users.items():\n",
    "    # Select a random server (node) for this application\n",
    "    server = random.choice(list(G.nodes()))\n",
    "    \n",
    "    # Get the total resource requirements for this application\n",
    "    container_list = [c for c in containers_data['Containers'] if c['application'] == app_name]\n",
    "    if container_list:\n",
    "        containers = container_list[0]\n",
    "    else:\n",
    "        print(f\"No containers found for the application {app_name}\")\n",
    "        continue  # or handle this case appropriately\n",
    "\n",
    "    total_cpu = sum(float(c['cpu']) for c in containers['vnfs'] + containers['microservices'])\n",
    "    total_mem = sum(float(c['memory']) for c in containers['vnfs'] + containers['microservices'])\n",
    "    total_storage = sum(float(c['storage']) for c in containers['vnfs'] + containers['microservices'])\n",
    "    \n",
    "    # Check server capacity and update server utilization\n",
    "    server_capacity = G.nodes[server]\n",
    "    if server_utilization_random[server]['cpu'] + total_cpu <= convert_capacity_to_float(server_capacity['cpu']) and \\\n",
    "        server_utilization_random[server]['memory'] + total_mem <= convert_capacity_to_float(server_capacity['memory']) and \\\n",
    "        server_utilization_random[server]['storage'] + total_storage <= convert_capacity_to_float(server_capacity['storage']):\n",
    "        server_utilization_random[server]['cpu'] += total_cpu\n",
    "        server_utilization_random[server]['memory'] += total_mem\n",
    "        server_utilization_random[server]['storage'] += total_storage\n",
    "        # Check if the server is an edge or cloud server and update the counter\n",
    "        if \"cloud\" in server.lower():\n",
    "            total_cloud_servers_used_random += 1\n",
    "        else:\n",
    "            total_edge_servers_used_random += 1\n",
    "    else:\n",
    "        print(f\"Server capacity exceeded for server {server}\")\n",
    "        continue\n",
    "\n",
    "    # Routing and link utilization\n",
    "    for user in users:\n",
    "        source_node = user_to_node_map[user]\n",
    "        bandwidth = G_users.nodes[user]['bandwidth']\n",
    "        path = find_feasible_path(G, source=source_node, target=server, bandwidth=bandwidth)\n",
    "        \n",
    "        if path:\n",
    "            # Update link utilization and total routing cost along this path\n",
    "            # for i in range(len(path) - 1):\n",
    "            #     link = (path[i], path[i + 1])\n",
    "            #     link_utilization_random[link] += bandwidth\n",
    "            #     total_routing_cost_random += G[path[i]][path[i + 1]]['cost']\n",
    "            #     accepted_users_count_random += 1\n",
    "            # Update link utilization and total routing cost along this path\n",
    "            user_propagation_delay = 0.0  # Initialize the user's total propagation delay to zero\n",
    "            for i in range(len(path) - 1):\n",
    "                link = (path[i], path[i + 1])\n",
    "                link_utilization_random[link] += bandwidth\n",
    "                link_cost = bandwidth * 1e-3 * routing_cost\n",
    "                #link_cost = bandwidth * 1e-3 * G[path[i]][path[i + 1]]['cost']  # bandwidth in Gbps * cost ($/Gb)\n",
    "                link_cost_random[link] += link_cost  # Accumulate cost for this link\n",
    "                #total_routing_cost_random += G[path[i]][path[i + 1]]['cost']\n",
    "                total_routing_cost_random += link_cost  # Update the total routing cost\n",
    "\n",
    "                # Add up the propagation delay for the current link to the user's total\n",
    "                user_propagation_delay += propagation_delays.get(link, 0)\n",
    "                accepted_users_count_random += 1\n",
    "             # Add this user's propagation delay to the total\n",
    "            total_propagation_delay_random += user_propagation_delay\n",
    "        else:\n",
    "            print(f\"Could not find feasible path for user {user}\")\n",
    "            rejected_users_count_random += 1\n",
    "\n",
    "# Print server and link utilization\n",
    "# print(\"Server Utilization:\")\n",
    "# for server, util in server_utilization_random.items():\n",
    "#     print(f\"Server {server}: CPU = {util['cpu']} GHz, Memory = {util['memory']} GB, Storage = {util['storage']} GB\")\n",
    "    \n",
    "# print(\"\\nLink Utilization:\")\n",
    "# for link, bandwidth in link_utilization_random.items():\n",
    "#     print(f\"Link {link}: {bandwidth} Mbps\")\n",
    "\n",
    "# Print total routing cost\n",
    "print(f\"\\nTotal Routing Cost: {total_routing_cost_random}\")\n",
    "\n",
    "#Print total accepted users\n",
    "print(f\"\\nTotal Accepted Users: {accepted_users_count_random}\")\n",
    "accepted_percentage_random = accepted_users_count_random/NoOfUsers*100\n",
    "print(f\"\\nPercentage of Accepted Users: {accepted_percentage_random}%\")\n",
    "\n",
    "\n",
    "#Print total rejected users\n",
    "print(f\"\\nTotal Rejected Users: {rejected_users_count_random}\")\n",
    "rejected_percentage_random = rejected_users_count_random/NoOfUsers*100\n",
    "print(f\"\\nPercentage of Rejected Users: {rejected_percentage_random}%\")\n",
    "\n",
    "\n",
    "def print_total_propagation_delay():\n",
    "    print(f\"\\nTotal Propagation Delay: {total_propagation_delay_random} ms\")\n",
    "\n",
    "print_total_propagation_delay()\n",
    "\n",
    "# Additional print statements for the counters\n",
    "print(f\"\\nTotal Edge Servers Used: {total_edge_servers_used_random}\")\n",
    "print(f\"\\nTotal Cloud Servers Used: {total_cloud_servers_used_random}\")\n",
    "\n",
    "total_cost_random = total_routing_cost_random + total_edge_servers_used_random*edge_server_cost + total_cloud_servers_used_random*cloud_server_cost\n",
    "\n",
    "print(f\"\\nTotal cost: {total_cost_random}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting Random Microservice Placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Server Utilization Bar Graph\n",
    "servers = list(server_utilization_random.keys())\n",
    "cpu_util = [server_utilization_random[s]['cpu'] for s in servers]\n",
    "mem_util = [server_utilization_random[s]['memory'] for s in servers]\n",
    "storage_util = [server_utilization_random[s]['storage'] for s in servers]\n",
    "\n",
    "barWidth = 0.25\n",
    "r1 = np.arange(len(cpu_util))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(r1, cpu_util, color='b', width=barWidth, edgecolor='grey', label='CPU (GHz)')\n",
    "plt.bar(r2, mem_util, color='c', width=barWidth, edgecolor='grey', label='Memory (GB)')\n",
    "plt.bar(r3, storage_util, color='m', width=barWidth, edgecolor='grey', label='Storage (GB)')\n",
    "\n",
    "plt.xlabel('Servers', fontweight='bold')\n",
    "plt.ylabel('Utilization')\n",
    "plt.title('Server Utilization')\n",
    "plt.xticks([r + barWidth for r in range(len(cpu_util))], servers)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#**********************************************************************************************************************\n",
    "\n",
    "# Link Utilization Bar Graph\n",
    "links = list(link_utilization_random.keys())\n",
    "bandwidth_util = [link_utilization_random[l] for l in links]\n",
    "link_names = [f\"{l[0]}-{l[1]}\" for l in links]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(link_names, bandwidth_util, color='r', edgecolor='grey')\n",
    "\n",
    "plt.xlabel('Links')\n",
    "plt.ylabel('Bandwidth (Mbps)')\n",
    "plt.title('Link Utilization')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#**********************************************************************************************************************\n",
    "\n",
    "# Create a new figure for Accepted and Rejected Users\n",
    "y_max = 100  # Adjust this value as needed\n",
    "\n",
    "# Create a new figure for Accepted and Rejected Users\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar positions for Accepted and Rejected Users\n",
    "r4 = [x + barWidth for x in r3]\n",
    "\n",
    "user_percentages = [accepted_percentage_random, rejected_percentage_random]\n",
    "user_labels = ['Accepted Users', 'Rejected Users']\n",
    "\n",
    "# Set the bar positions at the center of the ticks\n",
    "bar_positions = [r + barWidth / 2 for r in r4[:2]]\n",
    "\n",
    "plt.bar(bar_positions, user_percentages, color=['g', 'r'], width=barWidth, edgecolor='grey')\n",
    "plt.xlabel('User Status')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Percentage of Accepted vs. Rejected Users')\n",
    "\n",
    "# Set the x-axis ticks at the center of the bars\n",
    "plt.xticks(bar_positions, user_labels)\n",
    "\n",
    "# Set the y-axis limit slightly above 100%\n",
    "plt.ylim(0, y_max)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placement using Page Rank in Descending page rank order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize data structures\n",
    "server_utilization_pagerank = defaultdict(lambda: {'cpu': 0.0, 'memory': 0.0, 'storage': 0.0})\n",
    "link_utilization_pagerank = defaultdict(float)\n",
    "total_routing_cost_pagerank = 0.0\n",
    "total_propagation_delay_pagerank = 0.0\n",
    "accepted_users_count_pagerank = 0\n",
    "rejected_users_count_pagerank = 0\n",
    "\n",
    "total_edge_servers_used_pagerank = 0\n",
    "total_cloud_servers_used_pagerank = 0\n",
    "total_cost_pagerank = 0.0\n",
    "used_edge_servers = set()\n",
    "used_cloud_servers = set()\n",
    "\n",
    "\n",
    "# Page Rank Weights\n",
    "# Replace this with your actual PageRank data\n",
    "pagerank_df = PageRank.compute_pagerank(G)\n",
    "sorted_pagerank = pagerank_df.sort_values(by='value', ascending=False)\n",
    "\n",
    "# Function to calculate a composite score for server selection\n",
    "def server_score(pagerank, server_utilization):\n",
    "    return pagerank - 0.01 * (server_utilization['cpu'] + server_utilization['memory'] + server_utilization['storage'])\n",
    "\n",
    "\n",
    "\n",
    "# Function to convert string capacity to float (e.g., '3TB' to 3000.0)\n",
    "def convert_capacity_to_float(capacity_str):\n",
    "    if 'GHz' in capacity_str:\n",
    "        return float(capacity_str.replace('GHz', ''))\n",
    "    elif 'TB' in capacity_str:\n",
    "        return float(capacity_str.replace('TB', '')) * 1000  # Convert TB to GB\n",
    "    else:\n",
    "        return float(capacity_str)\n",
    "\n",
    "# Function to find a path that can handle the given bandwidth\n",
    "def find_feasible_path(G, source, target, bandwidth):\n",
    "    for path in nx.all_simple_paths(G, source=source, target=target, cutoff=50):  # cutoff is optional, for performance\n",
    "        can_use_path = True\n",
    "        for i in range(len(path) - 1):\n",
    "            link = (path[i], path[i + 1])\n",
    "            link_capacity = G[path[i]][path[i + 1]]['capacity']  # Assuming capacity is in Mbps\n",
    "            if link_utilization_pagerank[link] + bandwidth > link_capacity:\n",
    "                can_use_path = False\n",
    "                break\n",
    "        if can_use_path:\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "# Microservice Placement using PageRank\n",
    "for app_name, users in app_to_users.items():\n",
    "    \n",
    "    # Sort servers by PageRank and current utilization\n",
    "    #sorted_pagerank['server_score'] = sorted_pagerank.apply(lambda row: server_score(row['value'], server_utilization_pagerank[row['name']]), axis=1)\n",
    "    #sorted_servers = sorted_pagerank.sort_values(by='server_score', ascending=False)['name'].tolist()\n",
    "\n",
    "    #for server in sorted_servers:\n",
    "    for _, row in sorted_pagerank.iterrows():\n",
    "        server = row['name']\n",
    "        \n",
    "        # Get the total resource requirements for this application\n",
    "        container_list = [c for c in containers_data['Containers'] if c['application'] == app_name]\n",
    "        if container_list:\n",
    "            containers = container_list[0]\n",
    "        else:\n",
    "            print(f\"No containers found for the application {app_name}\")\n",
    "            break\n",
    "\n",
    "        total_cpu = sum(float(c['cpu']) for c in containers['vnfs'] + containers['microservices'])\n",
    "        total_mem = sum(float(c['memory']) for c in containers['vnfs'] + containers['microservices'])\n",
    "        total_storage = sum(float(c['storage']) for c in containers['vnfs'] + containers['microservices'])\n",
    "        \n",
    "        # Check server capacity\n",
    "        server_capacity = G.nodes[server]\n",
    "        if server_utilization_pagerank[server]['cpu'] + total_cpu <= convert_capacity_to_float(server_capacity['cpu']) and \\\n",
    "           server_utilization_pagerank[server]['memory'] + total_mem <= convert_capacity_to_float(server_capacity['memory']) and \\\n",
    "           server_utilization_pagerank[server]['storage'] + total_storage <= convert_capacity_to_float(server_capacity['storage']):\n",
    "            \n",
    "            # Update server utilization\n",
    "            server_utilization_pagerank[server]['cpu'] += total_cpu\n",
    "            server_utilization_pagerank[server]['memory'] += total_mem\n",
    "            server_utilization_pagerank[server]['storage'] += total_storage\n",
    "\n",
    "            # Update the total number of edge/cloud servers used\n",
    "            if \"cloud\" in server.lower():\n",
    "                used_cloud_servers.add(server)\n",
    "            else:\n",
    "               used_edge_servers.add(server)\n",
    "            break\n",
    "        else:\n",
    "            continue  # Move to next highest PageRank server\n",
    "\n",
    "    # Routing and link utilization\n",
    "    for user in users:\n",
    "        source_node = user_to_node_map[user]\n",
    "        bandwidth = G_users.nodes[user]['bandwidth']\n",
    "        path = find_feasible_path(G, source=source_node, target=server, bandwidth=bandwidth)\n",
    "        \n",
    "        if path:\n",
    "            user_propagation_delay = 0.0  # Initialize the user's total propagation delay to zero\n",
    "            for i in range(len(path) - 1):\n",
    "                link = (path[i], path[i + 1])\n",
    "                link_utilization_pagerank[link] += bandwidth\n",
    "                link_cost = bandwidth * 1e-3 * routing_cost\n",
    "                #total_routing_cost_pagerank += G[path[i]][path[i + 1]]['cost']\n",
    "                total_routing_cost_pagerank += link_cost\n",
    "                user_propagation_delay += propagation_delays.get(link, 0)  # Assuming you have a propagation_delays dict\n",
    "            \n",
    "            # Add this user's propagation delay to the total\n",
    "            total_propagation_delay_pagerank += user_propagation_delay\n",
    "            accepted_users_count_pagerank += 1\n",
    "        else:\n",
    "            print(f\"Could not find feasible path for user {user}\")\n",
    "            rejected_users_count_pagerank += 1\n",
    "\n",
    "# Print server and link utilization\n",
    "print(\"Server Utilization:\")\n",
    "for server, util in server_utilization_pagerank.items():\n",
    "    print(f\"Server {server}: CPU = {util['cpu']} GHz, Memory = {util['memory']} GB, Storage = {util['storage']} GB\")\n",
    "    \n",
    "print(\"\\nLink Utilization:\")\n",
    "for link, bandwidth in link_utilization_pagerank.items():\n",
    "    continue\n",
    "    #print(f\"Link {link}: {bandwidth} Mbps\")\n",
    "\n",
    "# Print total routing cost\n",
    "print(f\"\\nTotal Routing Cost: {total_routing_cost_pagerank}\")\n",
    "\n",
    "\n",
    "# Print total propagation delay\n",
    "print(f\"\\nTotal Propagation Delay: {total_propagation_delay_pagerank} ms\")\n",
    "\n",
    "# Print total accepted and rejected users\n",
    "print(f\"\\nTotal Accepted Users: {accepted_users_count_pagerank}\")\n",
    "accepted_percentage_pagerank = accepted_users_count_pagerank / NoOfUsers * 100\n",
    "print(f\"\\nPercentage of Accepted Users: {accepted_percentage_pagerank}%\")\n",
    "\n",
    "print(f\"\\nTotal Rejected Users: {rejected_users_count_pagerank}\")\n",
    "rejected_percentage_pagerank = rejected_users_count_pagerank / NoOfUsers * 100\n",
    "print(f\"\\nPercentage of Rejected Users: {rejected_percentage_pagerank}%\")\n",
    "\n",
    "total_edge_servers_used_pagerank = len(used_edge_servers)\n",
    "total_cloud_servers_used_pagerank = len(used_cloud_servers)\n",
    "\n",
    "# Additional print statements for the counters\n",
    "print(f\"\\nTotal Edge Servers Used: {total_edge_servers_used_pagerank}\")\n",
    "print(f\"\\nTotal Cloud Servers Used: {total_cloud_servers_used_pagerank}\")\n",
    "\n",
    "total_cost_pagerank = total_routing_cost_pagerank + total_edge_servers_used_pagerank*edge_server_cost + total_cloud_servers_used_pagerank*cloud_server_cost\n",
    "print(f\"\\nTotal cost: {total_cost_pagerank}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Debug Script: Print the state of all relevant variables for diagnostics\n",
    "\n",
    "# Ensure all necessary imports are present\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "\n",
    "# Assuming the existence of variables and data structures from your context\n",
    "\n",
    "# Print app_to_users content\n",
    "print(\"Debug - app_to_users content:\", app_to_users)\n",
    "\n",
    "\n",
    "# Print the graph G nodes and edges if needed\n",
    "print(\"\\nGraph G Nodes and Edges:\")\n",
    "print(\"Nodes:\", G.nodes(data=True))\n",
    "print(\"Edges:\", G.edges(data=True))\n",
    "\n",
    "# Print feasible paths (Assuming you have a way to collect these)\n",
    "# This part needs to be integrated with your path finding and processing logic\n",
    "# Here's an example placeholder for how you might structure this\n",
    "print(\"\\nFeasible Paths:\")\n",
    "# Assuming feasible_paths is a list of tuples (path, bandwidth)\n",
    "# You would need to modify your path finding to collect these\n",
    "feasible_paths = []  # Placeholder for where you would collect feasible paths\n",
    "for path, bandwidth in feasible_paths:\n",
    "    print(f\"Path: {path} with bandwidth {bandwidth} Mbps\")\n",
    "\n",
    "# Print any other specific variables you're interested in\n",
    "# For example, capacities, apps, users, etc.\n",
    "# Example placeholder for server capacities\n",
    "print(\"\\nServer Capacities:\")\n",
    "# Assuming a dictionary or graph node attribute for server capacities\n",
    "for server in G.nodes():\n",
    "    attrs = G.nodes[server]\n",
    "    print(f\"Server {server}: CPU = {attrs.get('cpu', 'N/A')}, Memory = {attrs.get('memory', 'N/A')}, Storage = {attrs.get('storage', 'N/A')}\")\n",
    "\n",
    "# Add similar sections for any other variables or data structures you wish to debug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have server_utilization, link_utilization, accepted_percentage, and rejected_percentage already defined\n",
    "\n",
    "# Server Utilization Bar Graph\n",
    "servers = list(server_utilization_pagerank.keys())\n",
    "cpu_util = [server_utilization_pagerank[s]['cpu'] for s in servers]\n",
    "mem_util = [server_utilization_pagerank[s]['memory'] for s in servers]\n",
    "storage_util = [server_utilization_pagerank[s]['storage'] for s in servers]\n",
    "\n",
    "barWidth = 0.25\n",
    "r1 = np.arange(len(cpu_util))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(r1, cpu_util, color='b', width=barWidth, edgecolor='grey', label='CPU (GHz)')\n",
    "plt.bar(r2, mem_util, color='c', width=barWidth, edgecolor='grey', label='Memory (GB)')\n",
    "plt.bar(r3, storage_util, color='m', width=barWidth, edgecolor='grey', label='Storage (GB)')\n",
    "\n",
    "plt.xlabel('Servers', fontweight='bold')\n",
    "plt.ylabel('Utilization')\n",
    "plt.title('Server Utilization')\n",
    "plt.xticks([r + barWidth for r in range(len(cpu_util))], servers)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Link Utilization Bar Graph\n",
    "links = list(link_utilization_pagerank.keys())\n",
    "bandwidth_util = [link_utilization_pagerank[l] for l in links]\n",
    "link_names = [f\"{l[0]}-{l[1]}\" for l in links]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(link_names, bandwidth_util, color='r', edgecolor='grey')\n",
    "\n",
    "plt.xlabel('Links')\n",
    "plt.ylabel('Bandwidth (Mbps)')\n",
    "plt.title('Link Utilization')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "# Accepted vs Rejected Users Bar Graph\n",
    "y_max = 100  # Adjust this value as needed\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "user_percentages = [accepted_percentage_pagerank, rejected_percentage_pagerank]\n",
    "user_labels = ['Accepted Users', 'Rejected Users']\n",
    "\n",
    "bar_positions = np.arange(len(user_labels))\n",
    "\n",
    "plt.bar(bar_positions, user_percentages, color=['g', 'r'], edgecolor='grey')\n",
    "\n",
    "plt.xlabel('User Status')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Percentage of Accepted vs. Rejected Users')\n",
    "plt.xticks(bar_positions, user_labels)\n",
    "plt.ylim(0, y_max)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive VNF Placement Algorithm\n",
    "\n",
    "1. *Determining Service Order Algorithm*: This algorithm takes several parameters like sets of users, SFCs, bandwidth requirements, and computational demands and times for CNFs. It calculates the urgency and criticality for each SFC and sorts them.\n",
    "2. *Placement Algorithm*: For each application in the sorted Service Order Queue, it tries to place the most critical VNF on the Central_Server. If that's not possible, it calculates a Candidate score for each node and sorts them to find the next best candidate.\n",
    "3. *Experimental Setup and Results*: You have graphs for the average number of servers used, SFC length, algorithm execution time, CPU utilization, and link utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize data structures to hold server and link states\n",
    "server_utilization_adaptive = defaultdict(lambda: {'cpu': 0.0, 'memory': 0.0, 'storage': 0.0})\n",
    "link_utilization_adaptive = defaultdict(float)\n",
    "link_cost_adaptive = defaultdict(float)\n",
    "total_routing_cost_adaptive = 0.0\n",
    "total_propagation_delay_adaptive = 0.0\n",
    "accepted_users_count_adaptive = 0\n",
    "rejected_users_count_adaptive = 0\n",
    "\n",
    "# Initialize additional counters\n",
    "total_edge_servers_used_adaptive = 0\n",
    "total_cloud_servers_used_adaptive = 0\n",
    "total_cost_adaptive = 0.0\n",
    "\n",
    "def convert_capacity_to_float(capacity_str):\n",
    "    if 'GHz' in capacity_str:\n",
    "        return float(capacity_str.replace('GHz', ''))\n",
    "    elif 'TB' in capacity_str:\n",
    "        return float(capacity_str.replace('TB', '')) * 1000\n",
    "    else:\n",
    "        return float(capacity_str)\n",
    "\n",
    "def find_feasible_path(G, source, target, bandwidth):\n",
    "    try:\n",
    "        path = nx.shortest_path(G, source=source, target=target, weight='cost')\n",
    "        can_use_path = all(\n",
    "            link_utilization_adaptive[(path[i], path[i + 1])] + bandwidth <= G[path[i]][path[i + 1]]['capacity']\n",
    "            for i in range(len(path) - 1)\n",
    "        )\n",
    "        if can_use_path:\n",
    "            return path\n",
    "    except nx.NetworkXNoPath:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def compute_link_cost(G, path, bandwidth):\n",
    "    cost = 0.0\n",
    "    for i in range(len(path) - 1):\n",
    "        link = (path[i], path[i + 1])\n",
    "        cost += bandwidth * 1e-3 * routing_cost\n",
    "        link_cost_adaptive[link] += cost\n",
    "    return cost\n",
    "\n",
    "# ... (your existing code for populating G, G_users, app_to_users, user_to_node_map, containers_data, propagation_delays)\n",
    "\n",
    "for app_name, users in app_to_users.items():\n",
    "    sorted_servers = sorted(G.nodes(), key=lambda x: server_utilization_adaptive[x]['cpu'])\n",
    "    \n",
    "    for server in sorted_servers:\n",
    "        container_list = [c for c in containers_data['Containers'] if c['application'] == app_name]\n",
    "        if not container_list:\n",
    "            print(f\"No containers found for the application {app_name}\")\n",
    "            break\n",
    "\n",
    "        containers = container_list[0]\n",
    "        total_cpu = sum(float(c['cpu']) for c in containers['vnfs'] + containers['microservices'])\n",
    "        total_mem = sum(float(c['memory']) for c in containers['vnfs'] + containers['microservices'])\n",
    "        total_storage = sum(float(c['storage']) for c in containers['vnfs'] + containers['microservices'])\n",
    "        \n",
    "        server_capacity = G.nodes[server]\n",
    "        if all(\n",
    "            server_utilization_adaptive[server][resource] + total <= convert_capacity_to_float(server_capacity[resource])\n",
    "            for resource, total in [('cpu', total_cpu), ('memory', total_mem), ('storage', total_storage)]\n",
    "        ):\n",
    "            for resource, total in [('cpu', total_cpu), ('memory', total_mem), ('storage', total_storage)]:\n",
    "                server_utilization_adaptive[server][resource] += total\n",
    "            \n",
    "            # Update the total number of edge/cloud servers used\n",
    "            if \"cloud\" in server.lower():\n",
    "                total_cloud_servers_used_adaptive += 1\n",
    "            else:\n",
    "                total_edge_servers_used_adaptive += 1\n",
    "            \n",
    "            break\n",
    "\n",
    "    for user in users:\n",
    "        source_node = user_to_node_map[user]\n",
    "        bandwidth = G_users.nodes[user]['bandwidth']\n",
    "        path = find_feasible_path(G, source=source_node, target=server, bandwidth=bandwidth)\n",
    "        \n",
    "        if path:\n",
    "            user_propagation_delay = 0.0\n",
    "            path_cost = compute_link_cost(G, path, bandwidth)\n",
    "            for i in range(len(path) - 1):\n",
    "                link = (path[i], path[i + 1])\n",
    "                link_utilization_adaptive[link] += bandwidth\n",
    "                total_routing_cost_adaptive += bandwidth * 1e-3 * routing_cost\n",
    "                user_propagation_delay += propagation_delays.get(link, 0)\n",
    "            total_propagation_delay_adaptive += user_propagation_delay\n",
    "            accepted_users_count_adaptive += 1\n",
    "        else:\n",
    "            print(f\"Could not find feasible path for user {user}\")\n",
    "            rejected_users_count_adaptive += 1\n",
    "\n",
    "# Additional print statements for the counters\n",
    "print(f\"\\nTotal Edge Servers Used: {total_edge_servers_used_adaptive}\")\n",
    "print(f\"\\nTotal Cloud Servers Used: {total_cloud_servers_used_adaptive}\")\n",
    "\n",
    "total_cost_adaptive = total_routing_cost_adaptive + total_edge_servers_used_adaptive*edge_server_cost + total_cloud_servers_used_adaptive*cloud_server_cost\n",
    "print(f\"\\nTotal cost: {total_cost_adaptive} $\")\n",
    "\n",
    "print(\"\\nCost for Each Link:\")\n",
    "for link, cost in link_cost_adaptive.items():\n",
    "    print(f\"Link {link}: {cost} $\")\n",
    "\n",
    "print(f\"\\nTotal routing cost: {total_routing_cost_adaptive} $\")\n",
    "print(f\"\\nTotal Propagation Delay: {total_propagation_delay_adaptive} ms\")\n",
    "print(f\"\\nTotal Accepted Users: {accepted_users_count_adaptive}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Placement Plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Server Utilization Bar Graph\n",
    "servers = list(server_utilization_adaptive.keys())\n",
    "cpu_util = [server_utilization_adaptive[s]['cpu'] for s in servers]\n",
    "mem_util = [server_utilization_adaptive[s]['memory'] for s in servers]\n",
    "storage_util = [server_utilization_adaptive[s]['storage'] for s in servers]\n",
    "\n",
    "barWidth = 0.25\n",
    "r1 = np.arange(len(cpu_util))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(r1, cpu_util, color='b', width=barWidth, edgecolor='grey', label='CPU (GHz)')\n",
    "plt.bar(r2, mem_util, color='c', width=barWidth, edgecolor='grey', label='Memory (GB)')\n",
    "plt.bar(r3, storage_util, color='m', width=barWidth, edgecolor='grey', label='Storage (GB)')\n",
    "\n",
    "plt.xlabel('Servers', fontweight='bold')\n",
    "plt.ylabel('Utilization')\n",
    "plt.title('Server Utilization')\n",
    "plt.xticks([r + barWidth for r in range(len(cpu_util))], servers)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Link Utilization Bar Graph\n",
    "links = list(link_utilization_adaptive.keys())\n",
    "bandwidth_util = [link_utilization_adaptive[l] for l in links]\n",
    "link_names = [f\"{l[0]}-{l[1]}\" for l in links]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(link_names, bandwidth_util, color='r', edgecolor='grey')\n",
    "\n",
    "plt.xlabel('Links')\n",
    "plt.ylabel('Bandwidth (Mbps)')\n",
    "plt.title('Link Utilization')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "# Accepted vs Rejected Users Bar Graph\n",
    "y_max = 100  # Adjust this value as needed\n",
    "\n",
    "accepted_percentage_adaptive = (accepted_users_count_adaptive / NoOfUsers) * 100\n",
    "rejected_percentage_adaptive = (rejected_users_count_adaptive / NoOfUsers) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "user_percentages = [accepted_percentage_adaptive, rejected_percentage_adaptive]\n",
    "user_labels = ['Accepted Users', 'Rejected Users']\n",
    "\n",
    "bar_positions = np.arange(len(user_labels))\n",
    "\n",
    "plt.bar(bar_positions, user_percentages, color=['g', 'r'], edgecolor='grey')\n",
    "\n",
    "plt.xlabel('User Status')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Percentage of Accepted vs. Rejected Users')\n",
    "plt.xticks(bar_positions, user_labels)\n",
    "plt.ylim(0, y_max)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Plots for Random Placement, Page Rank Placement and Adaptive Placement Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Server Utilization Plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Assuming you have server_utilization_adaptive, server_utilization_pagerank, server_utilization_random\n",
    "server_labels = ['Random', 'PageRank', 'Adaptive']\n",
    "\n",
    "# CPU Utilization\n",
    "cpu_util = [\n",
    "    np.mean([server_utilization_random[s]['cpu'] for s in server_utilization_random]),\n",
    "    np.mean([server_utilization_pagerank[s]['cpu'] for s in server_utilization_pagerank]),\n",
    "    np.mean([server_utilization_adaptive[s]['cpu'] for s in server_utilization_adaptive])\n",
    "]\n",
    "\n",
    "# Memory Utilization\n",
    "mem_util = [\n",
    "    np.mean([server_utilization_random[s]['memory'] for s in server_utilization_random]),\n",
    "    np.mean([server_utilization_pagerank[s]['memory'] for s in server_utilization_pagerank]),\n",
    "    np.mean([server_utilization_adaptive[s]['memory'] for s in server_utilization_adaptive])\n",
    "]\n",
    "\n",
    "# Storage Utilization\n",
    "storage_util = [\n",
    "    np.mean([server_utilization_random[s]['storage'] for s in server_utilization_random]),\n",
    "    np.mean([server_utilization_pagerank[s]['storage'] for s in server_utilization_pagerank]),\n",
    "    np.mean([server_utilization_adaptive[s]['storage'] for s in server_utilization_adaptive])\n",
    "]\n",
    "\n",
    "barWidth = 0.25\n",
    "r1 = np.arange(len(cpu_util))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "#*********************************************************************************************************************************************\n",
    "# Plotting CPU Utilization\n",
    "axs[0].bar(r1, cpu_util, color='b', width=barWidth, edgecolor='grey', label='CPU (GHz)')\n",
    "axs[0].set_title('Average CPU Utilization')\n",
    "axs[0].set_xticks([r + barWidth for r in range(len(cpu_util))])\n",
    "axs[0].set_xticklabels(server_labels)\n",
    "axs[0].legend()\n",
    "#*********************************************************************************************************************************************\n",
    "# Plotting Memory Utilization\n",
    "axs[1].bar(r1, mem_util, color='c', width=barWidth, edgecolor='grey', label='Memory (GB)')\n",
    "axs[1].set_title('Average Memory Utilization')\n",
    "axs[1].set_xticks([r + barWidth for r in range(len(mem_util))])\n",
    "axs[1].set_xticklabels(server_labels)\n",
    "axs[1].legend()\n",
    "#*********************************************************************************************************************************************\n",
    "# Plotting Storage Utilization\n",
    "axs[2].bar(r1, storage_util, color='m', width=barWidth, edgecolor='grey', label='Storage (GB)')\n",
    "axs[2].set_title('Average Storage Utilization')\n",
    "axs[2].set_xticks([r + barWidth for r in range(len(storage_util))])\n",
    "axs[2].set_xticklabels(server_labels)\n",
    "axs[2].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#*********************************************************************************************************************************************\n",
    "# Plotting Link Utilization\n",
    "plt.figure()\n",
    "avg_link_util = [\n",
    "    np.mean(list(link_utilization_random.values())),\n",
    "    np.mean(list(link_utilization_pagerank.values())),\n",
    "    np.mean(list(link_utilization_adaptive.values()))\n",
    "]\n",
    "\n",
    "plt.bar(server_labels, avg_link_util, color='r', edgecolor='grey')\n",
    "plt.title('Average Link Utilization')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Bandwidth (Mbps)')\n",
    "plt.show()\n",
    "\n",
    "#*********************************************************************************************************************************************\n",
    "\n",
    "# Plotting % of Accepted Users\n",
    "plt.figure()\n",
    "accepted_users_percentage = [\n",
    "    (accepted_users_count_random / (accepted_users_count_random + rejected_users_count_random)) * 100 if accepted_users_count_random + rejected_users_count_random > 0 else 0,\n",
    "    (accepted_users_count_pagerank / (accepted_users_count_pagerank + rejected_users_count_pagerank)) * 100 if accepted_users_count_pagerank + rejected_users_count_pagerank > 0 else 0,\n",
    "    (accepted_users_count_adaptive / (accepted_users_count_adaptive + rejected_users_count_adaptive)) * 100 if accepted_users_count_adaptive + rejected_users_count_adaptive > 0 else 0\n",
    "]\n",
    "\n",
    "plt.bar(server_labels, accepted_users_percentage, color='g', edgecolor='grey')\n",
    "plt.title('Percentage of Accepted Users')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.show()\n",
    "\n",
    "#*********************************************************************************************************************************************\n",
    "\n",
    "# Plotting % of Rejected Users\n",
    "plt.figure()\n",
    "rejected_users_percentage = [\n",
    "    (rejected_users_count_random / (accepted_users_count_random + rejected_users_count_random)) * 100 if accepted_users_count_random + rejected_users_count_random > 0 else 0,\n",
    "    (rejected_users_count_pagerank / (accepted_users_count_pagerank + rejected_users_count_pagerank)) * 100 if accepted_users_count_pagerank + rejected_users_count_pagerank > 0 else 0,\n",
    "    (rejected_users_count_adaptive / (accepted_users_count_adaptive + rejected_users_count_adaptive)) * 100 if accepted_users_count_adaptive + rejected_users_count_adaptive > 0 else 0\n",
    "]\n",
    "\n",
    "plt.bar(server_labels, rejected_users_percentage, color='g', edgecolor='grey')\n",
    "plt.title('Percentage of Rejected Users')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.show()\n",
    "\n",
    "#*********************************************************************************************************************************************\n",
    "# New Plot for Total Propagation Delay\n",
    "plt.figure()\n",
    "total_propagation_delay_values = [\n",
    "    total_propagation_delay_random,\n",
    "    total_propagation_delay_pagerank,\n",
    "    total_propagation_delay_adaptive\n",
    "]\n",
    "\n",
    "plt.bar(server_labels, total_propagation_delay_values, color='orange', edgecolor='grey')\n",
    "plt.title('Total Propagation Delay')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Delay (ms)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#*********************************************************************************************************************************************\n",
    "\n",
    "# New Plot for Total Routing Cost\n",
    "plt.figure()\n",
    "total_routing_cost_values = [\n",
    "    total_routing_cost_random,\n",
    "    total_routing_cost_pagerank,\n",
    "    total_routing_cost_adaptive\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(server_labels, total_routing_cost_values, color='purple', edgecolor='grey')\n",
    "\n",
    "plt.title('Total Routing Cost')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Cost (USD)')\n",
    "plt.show()\n",
    "#*********************************************************************************************************************************************\n",
    "\n",
    "#Plot for total cost\n",
    "\n",
    "plt.figure()\n",
    "total_cost_values = [\n",
    "    total_cost_random,\n",
    "    total_cost_pagerank,\n",
    "    total_cost_adaptive\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(server_labels, total_cost_values, color='purple', edgecolor='grey')\n",
    "\n",
    "plt.title('Total Cost')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Cost (USD)')\n",
    "plt.show()\n",
    "\n",
    "#*********************************************************************************************************************************************\n",
    "\n",
    "# Data\n",
    "algorithms = ['Random', 'PageRank', 'Adaptive']\n",
    "edge_servers = [\n",
    "    total_edge_servers_used_random,\n",
    "    total_edge_servers_used_pagerank,\n",
    "    total_edge_servers_used_adaptive\n",
    "]\n",
    "cloud_servers = [\n",
    "    total_cloud_servers_used_random,\n",
    "    total_cloud_servers_used_pagerank,\n",
    "    total_cloud_servers_used_adaptive\n",
    "]\n",
    "\n",
    "# Set up the bar chart\n",
    "x = np.arange(len(algorithms))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects1 = ax.bar(x - width/2, edge_servers, width, label='Edge Servers')\n",
    "rects2 = ax.bar(x + width/2, cloud_servers, width, label='Cloud Servers')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Algorithms')\n",
    "ax.set_title('Total Edge and Cloud Servers Used by Different Algorithms')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(algorithms)\n",
    "ax.legend()\n",
    "\n",
    "# Autolabel function to display the label on top of the bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Edge Servers Used (Random): \", total_edge_servers_used_random)\n",
    "print(\"Cloud Servers Used (Random): \", total_cloud_servers_used_random)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAMS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAMS Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_gams_model(gams_path, model_file, log_option, output_gdx):\n",
    "    \"\"\"\n",
    "    Run a GAMS model using Python.\n",
    "\n",
    "    Parameters:\n",
    "    gams_path (str): Full path to the GAMS executable.\n",
    "    model_file (str): The GAMS model file to run.\n",
    "    log_option (str): Log level option for GAMS.\n",
    "    input_gdx (str): GDX file for input data.\n",
    "    output_gdx (str): GDX file for output data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the command to run the GAMS model\n",
    "    command = f'\"{gams_path}\" {model_file} lo={log_option} gdx={output_gdx}'\n",
    "\n",
    "    # Run the command and capture output\n",
    "    try:\n",
    "        completed_process = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(\"GAMS model executed successfully.\")\n",
    "        print(completed_process.stdout.decode())\n",
    "        print(completed_process.stderr.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred in GAMS model execution.\")\n",
    "        print(e)\n",
    "        print(e.stdout.decode())\n",
    "        print(e.stderr.decode())\n",
    "\n",
    "# Path to your GAMS installation\n",
    "gams_path = 'C:\\\\GAMS\\\\win64\\\\24.9\\\\gams'\n",
    "\n",
    "# Your GAMS model file\n",
    "model_file = 'm2'\n",
    "\n",
    "# Define the path to your input GDX file\n",
    "input_gdx = 'Optimization_input.gdx'\n",
    "\n",
    "# Log option and output GDX file name\n",
    "log_option = '3'\n",
    "output_gdx = 'Optimization_output'\n",
    "\n",
    "# Run the GAMS model\n",
    "run_gams_model(gams_path, model_file, log_option, output_gdx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gams\n",
    "from gams import GamsWorkspace\n",
    "\n",
    "# Define paths to your GAMS installation and GAMS model file\n",
    "gams_path = 'C:\\\\GAMS\\\\win64\\\\24.9\\\\gams.exe'\n",
    "model_file = 'm2.gms'\n",
    "input_gdx = 'data.gdx'\n",
    "output_gdx = 'Optimization_output.gdx'\n",
    "\n",
    "# Get the absolute path to the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the absolute path to the output GDX file\n",
    "output_gdx_absolute = os.path.join(current_directory, output_gdx)\n",
    "\n",
    "# Function to verify GDX contents\n",
    "def verify_gdx_contents(gdx_file_path):\n",
    "    ws = gams.GamsWorkspace()\n",
    "    db = ws.add_database_from_gdx(gdx_file_path)\n",
    "    \n",
    "    for symbol in db:\n",
    "        print(f\"Symbol: {symbol.name}, Type: {type(symbol).__name__}\")\n",
    "        if isinstance(symbol, gams.GamsSet):\n",
    "            print(\"  Elements:\")\n",
    "            for record in symbol:\n",
    "                print(f\"    {record.key(0)}\")\n",
    "        elif isinstance(symbol, gams.GamsParameter):\n",
    "            print(\"  Values:\")\n",
    "            if len(symbol.domains) == 2:  # Check if the parameter has two-dimensional indices\n",
    "                for record in symbol:\n",
    "                    print(f\"    {record.keys[0]}-{record.keys[1]}: {record.value}\")\n",
    "            elif symbol.name == \"Topology\":  # Check if the parameter is 'Topology'\n",
    "                for record in symbol:\n",
    "                    print(f\"    {record.key(0)}-{record.keys[0]}: {record.value}\")\n",
    "            else:\n",
    "                try:\n",
    "                    for record in symbol:\n",
    "                        print(f\"    {record.key(0)}: {record.value}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error accessing values: {e}\")\n",
    "        elif isinstance(symbol, gams.GamsVariable):\n",
    "            print(\"  Values:\")\n",
    "            try:\n",
    "                for record in symbol:\n",
    "                    print(f\"    {record.key}: {record.level}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error accessing levels: {e}\")\n",
    "        else:\n",
    "            print(\"  Unknown type\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Verify the output GDX file contents\n",
    "verify_gdx_contents(output_gdx_absolute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import networkx as nx\n",
    "from gams import GamsWorkspace\n",
    "\n",
    "# Define paths and settings\n",
    "gams_system_directory = 'C:\\\\GAMS\\\\win64\\\\24.9'\n",
    "gdx_file_path = 'data.gdx'\n",
    "output_gdx = 'Optimization_output.gdx'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Initialize GAMS workspace\n",
    "ws = GamsWorkspace(system_directory=gams_system_directory, working_directory=current_directory)\n",
    "\n",
    "# Verify GDX contents function\n",
    "def verify_gdx_contents(gdx_file_path):\n",
    "    db = ws.add_database_from_gdx(gdx_file_path)\n",
    "    \n",
    "    for symbol in db:\n",
    "        print(f\"Symbol: {symbol.name}, Type: {type(symbol).__name__}\")\n",
    "        if isinstance(symbol, gams.GamsSet):\n",
    "            print(\"  Elements:\")\n",
    "            for record in symbol:\n",
    "                print(f\"    {record.key(0)}\")\n",
    "        elif isinstance(symbol, gams.GamsParameter):\n",
    "            print(\"  Values:\")\n",
    "            if len(symbol.domains) == 2:  # Check if the parameter has two-dimensional indices\n",
    "                for record in symbol:\n",
    "                    print(f\"    {record.keys[0]}-{record.keys[1]}: {record.value}\")\n",
    "            else:\n",
    "                try:\n",
    "                    for record in symbol:\n",
    "                        print(f\"    {record.key(0)}: {record.value}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error accessing values: {e}\")\n",
    "        else:\n",
    "            print(\"  Unknown type\")\n",
    "\n",
    "# Export data from GDX to Excel\n",
    "def export_to_excel(gdxxrw_path, gdx_file_path, excel_file_path):\n",
    "    try:\n",
    "        subprocess.run([gdxxrw_path, gdx_file_path, f\"par={excel_file_path}\", \"rng=A1\", \"wbt=par\", \"wb=par\"], check=True)\n",
    "        print(\"Data exported successfully from GDX to Excel.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error exporting data from GDX to Excel: {e}\")\n",
    "\n",
    "# Read data from GDX file, process it, and export results back to GDX\n",
    "def process_gdx_data(gdx_file_path, output_file_path):\n",
    "    db = ws.add_database()\n",
    "    db.execute_load(gdx_file_path)\n",
    "\n",
    "    # Process data, perform calculations, etc. (as in your script)\n",
    "\n",
    "    # Export results back to GDX file\n",
    "    db_out = ws.add_database()\n",
    "    # Add results to db_out\n",
    "    db_out.export(output_file_path)\n",
    "    print(\"Results exported successfully to GDX.\")\n",
    "\n",
    "# Verify GDX contents\n",
    "verify_gdx_contents(gdx_file_path)\n",
    "\n",
    "# Export data from GDX to Excel\n",
    "gdxxrw_path = os.path.join(gams_system_directory, 'gdxxrw.exe')\n",
    "excel_file_path = 'input.xlsx'\n",
    "export_to_excel(gdxxrw_path, gdx_file_path, excel_file_path)\n",
    "\n",
    "# Process GDX data and export results back to GDX\n",
    "process_gdx_data(gdx_file_path, output_gdx)\n",
    "\n",
    "\n",
    "\n",
    "# Get symbols from the GDX file\n",
    "topology_data = db.get_symbol(\"Topology\")\n",
    "link_delay_data = db.get_symbol(\"LinkDelay\")\n",
    "app_definition_data = db.get_symbol(\"App_Definition\")\n",
    "cpu_req_data = db.get_symbol(\"CPU_Req\")\n",
    "bw_req_data = db.get_symbol(\"BW_Req\")\n",
    "users_ap_data = db.get_symbol(\"Users_AP\")\n",
    "users_app_data = db.get_symbol(\"Users_App\")\n",
    "host_utilization_cap_data = db.get_symbol(\"HostUtilizationCap\")\n",
    "scs_data = db.get_symbol(\"SCs\")\n",
    "m_data = db.get_symbol(\"M\")\n",
    "link_cap_data = db.get_symbol(\"LinkCap\")\n",
    "user_bw_data = db.get_symbol(\"UserBW\")\n",
    "\n",
    "# Populate parameters\n",
    "topology = {element[0]: element[1] for element in topology_data}\n",
    "link_delay = {link[0]: link[1] for link in link_delay_data}\n",
    "app_definition = {app[0]: app[1] for app in app_definition_data}\n",
    "cpu_req = {node[0]: node[1] for node in cpu_req_data}\n",
    "bw_req = {user_node[0]: user_node[1] for user_node in bw_req_data}\n",
    "users_ap = {user_node[0]: user_node[1] for user_node in users_ap_data}\n",
    "users_app = {user_app[0]: user_app[1] for user_app in users_app_data}\n",
    "host_utilization_cap = {node[0]: node[1] for node in host_utilization_cap_data}\n",
    "scs = {sc[0]: sc[1] for sc in scs_data}\n",
    "m = {key[0]: key[1] for key in m_data}\n",
    "link_cap = {link[0]: link[1] for link in link_cap_data}\n",
    "user_bw = {user[0]: user[1] for user in user_bw_data}\n",
    "\n",
    "# Initialize data structures to hold server and link states\n",
    "server_utilization = defaultdict(lambda: {'cpu': 0.0, 'memory': 0.0, 'storage': 0.0})\n",
    "link_utilization = defaultdict(float)\n",
    "accepted_users_count = 0\n",
    "rejected_users_count = 0\n",
    "total_routing_cost = 0.0\n",
    "link_cost = defaultdict(float)\n",
    "total_propagation_delay = 0.0\n",
    "total_edge_servers_used = 0\n",
    "total_cloud_servers_used = 0\n",
    "total_cost = 0\n",
    "\n",
    "# Define a function to find a feasible path for a user\n",
    "def find_feasible_path(G, source, target, bandwidth):\n",
    "    for path in nx.all_simple_paths(G, source=source, target=target, cutoff=10):  # cutoff is optional, for performance\n",
    "        can_use_path = True\n",
    "        for i in range(len(path) - 1):\n",
    "            link = (path[i], path[i + 1])\n",
    "            if link_utilization[link] + bandwidth > link_cap[link]:\n",
    "                can_use_path = False\n",
    "                break\n",
    "        if can_use_path:\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "# Process each user\n",
    "for user, app in users_app.items():\n",
    "    user_node = users_ap[user]  # Get the node associated with the user\n",
    "    bandwidth = user_bw[user]  # Get the bandwidth requirement for the user\n",
    "\n",
    "    # Check if the user's node has sufficient CPU capacity\n",
    "    if server_utilization[user_node]['cpu'] + cpu_req[user_node] <= host_utilization_cap[user_node]:\n",
    "        # Find a feasible path for the user\n",
    "        path = find_feasible_path(topology, user_node, app_definition[app], bandwidth)\n",
    "        if path:\n",
    "            # Update link utilization and total routing cost along this path\n",
    "            for i in range(len(path) - 1):\n",
    "                link = (path[i], path[i + 1])\n",
    "                link_utilization[link] += bandwidth\n",
    "                total_routing_cost += link_delay[link] * bandwidth\n",
    "            # Update server utilization\n",
    "            server_utilization[user_node]['cpu'] += cpu_req[user_node]\n",
    "            accepted_users_count += 1\n",
    "        else:\n",
    "            rejected_users_count += 1\n",
    "    else:\n",
    "        rejected_users_count += 1\n",
    "\n",
    "# Calculate total propagation delay\n",
    "for user in users_ap.keys():\n",
    "    user_node = users_ap[user]\n",
    "    user_app = users_app[user]\n",
    "    propagation_delay = link_delay.get((user_node, app_definition[user_app]), 0)\n",
    "    total_propagation_delay += propagation_delay\n",
    "\n",
    "# Calculate total edge and cloud servers used\n",
    "total_edge_servers_used = sum(1 for node in server_utilization if 'cloud' not in node.lower())\n",
    "total_cloud_servers_used = sum(1 for node in server_utilization if 'cloud' in node.lower())\n",
    "\n",
    "# Calculate total cost\n",
    "total_cost = total_routing_cost + sum(host_utilization_cap.values())\n",
    "\n",
    "# Write results back to a GDX file if needed\n",
    "output_file_path = output_gdx\n",
    "db_out = ws.add_database()\n",
    "accepted_users_set = db_out.add_set(\"AcceptedUsers\")\n",
    "rejected_users_set = db_out.add_set(\"RejectedUsers\")\n",
    "total_prop_delay_set = db_out.add_set(\"TotalPropDelay\")\n",
    "total_edge_servers_set = db_out.add_set(\"TotalEdgeServers\")\n",
    "total_cloud_servers_set = db_out.add_set(\"TotalCloudServers\")\n",
    "total_cost_set = db_out.add_set(\"TotalCost\")\n",
    "accepted_users_set.add_record().value = accepted_users_count\n",
    "rejected_users_set.add_record().value = rejected_users_count\n",
    "total_prop_delay_set.add_record().value = total_propagation_delay\n",
    "total_edge_servers_set.add_record().value = total_edge_servers_used\n",
    "total_cloud_servers_set.add_record().value = total_cloud_servers_used\n",
    "total_cost_set.add_record().value = total_cost\n",
    "db_out.export(output_file_path)\n",
    "\n",
    "# Print some results\n",
    "print(\"User Acceptance Results:\")\n",
    "print(f\"Accepted Users: {accepted_users_count}\")\n",
    "print(f\"Rejected Users: {rejected_users_count}\")\n",
    "print(f\"Total Propagation Delay: {total_propagation_delay} ms\")\n",
    "print(f\"Total Edge Servers Used: {total_edge_servers_used}\")\n",
    "print(f\"Total Cloud Servers Used: {total_cloud_servers_used}\")\n",
    "print(f\"Total Cost: {total_cost}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test GDX File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gams import GamsWorkspace, GamsParameter\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "\n",
    "# Define paths and settings\n",
    "gams_system_directory = 'C:\\\\GAMS\\\\win64\\\\24.9'\n",
    "gdx_file_path = 'data.gdx'\n",
    "output_gdx = 'Optimization_output.gdx'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Initialize GAMS workspace\n",
    "ws = GamsWorkspace(system_directory=gams_system_directory, working_directory=current_directory)\n",
    "\n",
    "# Read data from the GDX file and print contents\n",
    "db = ws.add_database_from_gdx(gdx_file_path)\n",
    "print(\"Contents of the GDX file before creating a new GDX file:\")\n",
    "for symbol in db:\n",
    "    print(f\"Symbol: {symbol.name}\")\n",
    "    for record in symbol:\n",
    "        print(record)\n",
    "    print()\n",
    "\n",
    "\n",
    "#\n",
    "# Read data from the GDX file and print contents\n",
    "db = ws.add_database_from_gdx(gdx_file_path)\n",
    "for symbol in db:\n",
    "    print(f\"Symbol: {symbol.name}\")\n",
    "    for record in symbol:\n",
    "        print(record)\n",
    "    print()\n",
    "\n",
    "# Get symbols from the GDX file\n",
    "topology_data = db.get_symbol(\"Topology\")\n",
    "link_delay_data = db.get_symbol(\"LinkDelay\")\n",
    "cpu_req_data = db.get_symbol(\"CPU_Req\")\n",
    "bw_req_data = db.get_symbol(\"BW_Req\")\n",
    "users_ap_data = db.get_symbol(\"Users_AP\")\n",
    "users_app_data = db.get_symbol(\"Users_App\")\n",
    "host_utilization_cap_data = db.get_symbol(\"HostUtilizationCap\")\n",
    "link_cap_data = db.get_symbol(\"LinkCap\")\n",
    "user_bw_data = db.get_symbol(\"UserBW\")\n",
    "\n",
    "\n",
    "# Initialize data structures to hold server and link states\n",
    "server_utilization_adaptive = defaultdict(lambda: {'cpu': 0.0, 'memory': 0.0, 'storage': 0.0})\n",
    "link_utilization_adaptive = defaultdict(float)\n",
    "link_cost_adaptive = defaultdict(float)\n",
    "total_routing_cost_adaptive = 0.0\n",
    "total_propagation_delay_adaptive = 0.0\n",
    "accepted_users_count_adaptive = 0\n",
    "rejected_users_count_adaptive = 0\n",
    "\n",
    "# Initialize additional counters\n",
    "total_edge_servers_used_adaptive = 0\n",
    "total_cloud_servers_used_adaptive = 0\n",
    "total_cost_adaptive = 0.0\n",
    "\n",
    "# Populate parameters\n",
    "topology = {element[0]: element[1] for element in topology_data}\n",
    "link_delay = {link[0]: link[1] for link in link_delay_data}\n",
    "cpu_req = {node[0]: node[1] for node in cpu_req_data}\n",
    "bw_req = {user_node[0]: user_node[1] for user_node in bw_req_data}\n",
    "users_ap = {user_node[0]: user_node[1] for user_node in users_ap_data}\n",
    "users_app = {user_app[0]: user_app[1] for user_app in users_app_data}\n",
    "host_utilization_cap = {node[0]: node[1] for node in host_utilization_cap_data}\n",
    "link_cap = {link[0]: link[1] for link in link_cap_data}\n",
    "user_bw = {user[0]: user[1] for user in user_bw_data}\n",
    "\n",
    "# Define a function to find a feasible path for a user\n",
    "def find_feasible_path(G, source, target, bandwidth):\n",
    "    for path in nx.all_simple_paths(G, source=source, target=target, cutoff=10):  # cutoff is optional, for performance\n",
    "        can_use_path = True\n",
    "        for i in range(len(path) - 1):\n",
    "            link = (path[i], path[i + 1])\n",
    "            if link_utilization_adaptive[link] + bandwidth > link_cap[link]:\n",
    "                can_use_path = False\n",
    "                break\n",
    "        if can_use_path:\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "# Process each user\n",
    "for user, app in users_app.items():\n",
    "    user_node = users_ap[user]  # Get the node associated with the user\n",
    "    bandwidth = user_bw[user]  # Get the bandwidth requirement for the user\n",
    "\n",
    "    # Check if the user's node has sufficient CPU capacity\n",
    "    if server_utilization_adaptive[user_node]['cpu'] + cpu_req[user_node] <= host_utilization_cap[user_node]:\n",
    "        # Find a feasible path for the user\n",
    "        path = find_feasible_path(topology, user_node, app, bandwidth)\n",
    "        if path:\n",
    "            # Update link utilization and total routing cost along this path\n",
    "            for i in range(len(path) - 1):\n",
    "                link = (path[i], path[i + 1])\n",
    "                link_utilization_adaptive[link] += bandwidth\n",
    "                total_routing_cost_adaptive += link_delay[link] * bandwidth\n",
    "            # Update server utilization\n",
    "            server_utilization_adaptive[user_node]['cpu'] += cpu_req[user_node]\n",
    "            accepted_users_count_adaptive += 1\n",
    "        else:\n",
    "            rejected_users_count_adaptive += 1\n",
    "    else:\n",
    "        rejected_users_count_adaptive += 1\n",
    "\n",
    "# Calculate total propagation delay\n",
    "for user in users_ap.keys():\n",
    "    user_node = users_ap[user]\n",
    "    app = users_app[user]\n",
    "    propagation_delay = link_delay.get((user_node, app), 0)\n",
    "    total_propagation_delay_adaptive += propagation_delay\n",
    "\n",
    "# Calculate total edge and cloud servers used\n",
    "total_edge_servers_used_adaptive = sum(1 for node in server_utilization_adaptive if 'cloud' not in node.lower())\n",
    "total_cloud_servers_used_adaptive = sum(1 for node in server_utilization_adaptive if 'cloud' in node.lower())\n",
    "\n",
    "# Calculate total cost\n",
    "total_cost_adaptive = total_routing_cost_adaptive + sum(host_utilization_cap.values())\n",
    "\n",
    "# Write results back to a GDX file\n",
    "output_file_path = output_gdx\n",
    "db_out = ws.add_database()\n",
    "accepted_users_set = db_out.add_set(\"AcceptedUsers\")\n",
    "rejected_users_set = db_out.add_set(\"RejectedUsers\")\n",
    "total_prop_delay_set = db_out.add_set(\"TotalPropDelay\")\n",
    "total_edge_servers_set = db_out.add_set(\"TotalEdgeServers\")\n",
    "total_cloud_servers_set = db_out.add_set(\"TotalCloudServers\")\n",
    "total_cost_set = db_out.add_set(\"TotalCost\")\n",
    "accepted_users_set.add_record().value = accepted_users_count_adaptive\n",
    "rejected_users_set.add_record().value = rejected_users_count_adaptive\n",
    "total_prop_delay_set.add_record().value = total_propagation_delay_adaptive\n",
    "total_edge_servers_set.add_record().value = total_edge_servers_used_adaptive\n",
    "total_cloud_servers_set.add_record().value = total_cloud_servers_used_adaptive\n",
    "total_cost_set.add_record().value = total_cost_adaptive\n",
    "db_out.export(output_file_path)\n",
    "\n",
    "# Print some results\n",
    "print(\"User Acceptance Results:\")\n",
    "print(f\"Accepted Users: {accepted_users_count_adaptive}\")\n",
    "print(f\"Rejected Users: {rejected_users_count_adaptive}\")\n",
    "print(f\"Total Propagation Delay: {total_propagation_delay_adaptive} ms\")\n",
    "print(f\"Total Edge Servers Used: {total_edge_servers_used_adaptive}\")\n",
    "print(f\"Total Cloud Servers Used: {total_cloud_servers_used_adaptive}\")\n",
    "print(f\"Total Cost: {total_cost_adaptive}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the GAMS Python API path to sys.path\n",
    "gams_api_path = \"C:\\\\GAMS\\\\win64\\\\24.9\\\\apifiles\\\\Python\\\\api_36\"\n",
    "sys.path.append(gams_api_path)\n",
    "from gams import *\n",
    "\n",
    "\n",
    "# Initialize GAMS workspace and database\n",
    "ws = GamsWorkspace()\n",
    "db = ws.add_database()\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "apps_data = ['App1', 'App2', 'App3']\n",
    "app_definition_data = {(1, 'App1'): 1, (2, 'App2'): 1, (3, 'App3'): 1}  # Assuming (VNF, App): Value format\n",
    "bw_req_data = {'App1': 100, 'App2': 200, 'App3': 300}\n",
    "\n",
    "# Create GAMS sets and parameters\n",
    "apps_set = db.add_set('Apps', 1, 'Set of IoT Apps')\n",
    "app_def_param = db.add_parameter_dc('App_Definition', [apps_set], 'VNFs -> Apps relation')\n",
    "bw_req_param = db.add_parameter('BW_Req', 1, 'Bandwidth Requirement')\n",
    "\n",
    "# Populate data\n",
    "for app in apps_data:\n",
    "    apps_set.add_record(app)\n",
    "\n",
    "for (vnf, app), val in app_definition_data.items():\n",
    "    app_def_param.add_record(app).value = val\n",
    "\n",
    "for app, bw in bw_req_data.items():\n",
    "    bw_req_param.add_record(app).value = bw\n",
    "\n",
    "# Write to GDX file\n",
    "gdx_filename = 'Optimization_input.gdx'\n",
    "db.export(gdx_filename)\n",
    "\n",
    "print(f'GDX file created: {gdx_filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create json File - Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "# Assuming your data (G, G_users, app_to_users, containers_data) is already loaded or computed\n",
    "# For demonstration, these will be empty or simple structures\n",
    "# G = nx.Graph()\n",
    "# G_users = nx.Graph()\n",
    "# app_to_users = {}  # Example: {'App1': ['User1', 'User2'], 'App2': ['User3']}\n",
    "# containers_data = {}  # Example: {'Containers': [{'application': 'App1', 'vnfs': [{'name': 'VNF1'}]}]}\n",
    "\n",
    "# Step 1: Extract data\n",
    "# Extract Apps and Users\n",
    "apps = list(app_to_users.keys())\n",
    "users = list(G_users.nodes)\n",
    "\n",
    "# Extract Servers and VNFs\n",
    "servers = list(G.nodes)\n",
    "vnfs = set()  # To avoid duplicates\n",
    "for container in containers_data.get('Containers', []):\n",
    "    for vnf in container.get('vnfs', []):\n",
    "        vnfs.add(vnf['name'])\n",
    "\n",
    "# Step 2: Format data for GAMS\n",
    "# Convert to GAMS-friendly format (e.g., lists, dictionaries)\n",
    "\n",
    "# Example: Create a dictionary for App_Definition\n",
    "app_definition = {}\n",
    "for app, vnf_list in app_to_users.items():\n",
    "    for vnf in vnf_list:\n",
    "        app_definition[(vnf, app)] = 1  # or other relevant value\n",
    "\n",
    "# Initialize a new dictionary for the adjusted structure\n",
    "app_definition_adjusted = {}\n",
    "\n",
    "# Loop through the original app_definition dictionary\n",
    "for key, value in app_definition.items():\n",
    "    user, app = key  # Unpack the tuple\n",
    "    if user not in app_definition_adjusted:\n",
    "        app_definition_adjusted[user] = {}\n",
    "    app_definition_adjusted[user][app] = value\n",
    "\n",
    "# Step 3: Write data to a file (GDX or text)\n",
    "# For simplicity, we'll write to a JSON file (GDX writing requires GAMS Python API)\n",
    "\n",
    "data_for_gams = {\n",
    "    'Apps': apps,\n",
    "    'Users': users,\n",
    "    'Servers': servers,\n",
    "    'VNFs': list(vnfs),\n",
    "    'AppDefinition': app_definition_adjusted\n",
    "    # Add other parameters as needed\n",
    "}\n",
    "\n",
    "# Function to convert tuple keys to string keys\n",
    "def convert_tuple_keys_to_string(data):\n",
    "    new_data = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(key, tuple):\n",
    "            # Convert the tuple to a string, e.g., \"('user_0', 'City Surveillance')\" \n",
    "            string_key = str(key)  \n",
    "        else:\n",
    "            string_key = key\n",
    "        new_data[string_key] = value\n",
    "    return new_data\n",
    "\n",
    "def convert_tuple_keys_to_strings(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {str(key): convert_tuple_keys_to_strings(value) for key, value in data.items()}\n",
    "    return data\n",
    "\n",
    "\n",
    "# Convert the tuple keys in your data dictionary\n",
    "data_for_gams_converted = {k: convert_tuple_keys_to_string(v) if isinstance(v, dict) else v for k, v in data_for_gams.items()}\n",
    "\n",
    "# Write the converted data to a JSON file\n",
    "with open('data_for_gams_converted.json', 'w') as f:\n",
    "    json.dump(data_for_gams_converted, f, indent=4)\n",
    "\n",
    "# Note: You'll need to read this JSON in GAMS or use the GAMS Python API to create a GDX file\n",
    "\n",
    "# Continue from the previous code\n",
    "\n",
    "# Step 4: Additional data processing for GAMS parameters\n",
    "\n",
    "\n",
    "def get_user_ap(user, G_users):\n",
    "    # Retrieve the associated AP for the user from the graph\n",
    "    return G_users.nodes[user].get('associated_ap')  # Replace 'associated_ap' with the actual attribute name\n",
    "\n",
    "def get_server_capacity(server, G):\n",
    "    # Retrieve the capacity of the server from the graph\n",
    "    return G.nodes[server].get('capacity')  # Replace 'capacity' with the actual attribute name\n",
    "\n",
    "def get_link_delay(u, v, G):\n",
    "    # Retrieve the delay of the link from the graph\n",
    "    return G[u][v].get('delay')  # Replace 'delay' with the actual attribute name\n",
    "\n",
    "\n",
    "# Example: Users_AP and Users_App\n",
    "users_ap = {}\n",
    "users_app = {}\n",
    "for user in users:\n",
    "    # Assuming function get_user_ap returns the associated AP for the user\n",
    "    ap = get_user_ap(user, G_users)  # Replace with actual function/logic\n",
    "    app = G_users.nodes[user]['application']  # Replace with actual data structure\n",
    "    users_ap[(user, ap)] = 1  # Indicates user is connected to this AP\n",
    "    users_app[(user, app)] = 1  # Indicates user is using this app\n",
    "\n",
    "# Example: HostUtilizationCap\n",
    "host_utilization_cap = {}\n",
    "for server in servers:\n",
    "    # Assuming function get_server_capacity returns the capacity for the server\n",
    "    capacity = get_server_capacity(server, G)  # Replace with actual function/logic\n",
    "    host_utilization_cap[server] = capacity  # Replace with actual value\n",
    "\n",
    "\n",
    "\n",
    "# Example: Topology and LinkDelay\n",
    "topology = {}\n",
    "link_delay = {}\n",
    "for u, v in G.edges:\n",
    "    topology[(u, v)] = 1  # Example: 1 indicates a link exists\n",
    "    # Assuming function get_link_delay returns the delay for the link\n",
    "    delay = get_link_delay(u, v, G)  # Replace with actual function/logic\n",
    "    link_delay[(u, v)] = delay\n",
    "\n",
    "# Adjusting Users_AP\n",
    "users_ap_adjusted = {}\n",
    "for key, value in users_ap.items():\n",
    "    user, ap = key\n",
    "    if user not in users_ap_adjusted:\n",
    "        users_ap_adjusted[user] = {}\n",
    "    users_ap_adjusted[user][ap] = value\n",
    "\n",
    "# Adjusting Users_App\n",
    "users_app_adjusted = {}\n",
    "for key, value in users_app.items():\n",
    "    user, app = key\n",
    "    if user not in users_app_adjusted:\n",
    "        users_app_adjusted[user] = {}\n",
    "    users_app_adjusted[user][app] = value\n",
    "\n",
    "# Adjusting Topology\n",
    "topology_adjusted = {}\n",
    "for key, value in topology.items():\n",
    "    u, v = key\n",
    "    topology_adjusted[f\"{u}-{v}\"] = value  # Concatenate with a delimiter\n",
    "\n",
    "# Adjusting LinkDelay\n",
    "link_delay_adjusted = {}\n",
    "for key, value in link_delay.items():\n",
    "    u, v = key\n",
    "    #link_delay_adjusted[f\"{u}-{v}\"] = value  # Concatenate with a delimiter\n",
    "    #link_delay_adjusted[f\"{u}-{v}\"] = 100  # Concatenate with a delimiter\n",
    "    link_delay_adjusted = {f\"{u}-{v}\": propagation_delays.get((u, v)) for u, v in G.edges()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Add additional data to the dictionary for GAMS\n",
    "data_for_gams.update({\n",
    "    'Users_AP': users_ap_adjusted,\n",
    "    'Users_App': users_app_adjusted,\n",
    "    'HostUtilizationCap': host_utilization_cap,\n",
    "    'Topology': topology_adjusted,\n",
    "    'LinkDelay': link_delay_adjusted\n",
    "    # Add other parameters as needed\n",
    "})\n",
    "\n",
    "# Set the host utilization capacity to 100 for each server\n",
    "for server in data_for_gams['HostUtilizationCap'].keys():\n",
    "    data_for_gams['HostUtilizationCap'][server] = 100\n",
    "\n",
    "\n",
    "# Convert tuple keys to strings in the data_for_gams dictionary\n",
    "data_for_gams_converted = convert_tuple_keys_to_strings(data_for_gams)\n",
    "\n",
    "# Write the converted data to the JSON file\n",
    "# Convert the data_for_gams dictionary to JSON\n",
    "with open('data_for_gams_converted.json', 'w') as f:\n",
    "    json.dump(data_for_gams, f, indent=4)\n",
    "\n",
    "\n",
    "# Note: The functions get_user_ap, get_server_capacity, and get_link_delay are placeholders\n",
    "# and should be replaced with your actual data retrieval logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read json file and create GDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gams import *\n",
    "import os\n",
    "import subprocess  # Needed to call GAMS executable\n",
    "\n",
    "# Assuming the JSON loading and GAMS database population code remains the same\n",
    "\n",
    "# Define the path to the GAMS executable\n",
    "gams_executable_path = \"C:\\\\GAMS\\\\win64\\\\25.1\\\\gams.exe\"  # Adjust this to your GAMS installation path\n",
    "\n",
    "# Define the GAMS model file (assuming it's in the same directory as this script)\n",
    "gams_model_file = \"vnf_placement.gms\"  # Make sure this matches your GAMS model file name\n",
    "\n",
    "# Define the output directory for GAMS run\n",
    "gams_output_dir = \"C:\\\\Users\\\\youse\\\\Documents\\\\Github\\\\VNF-python\"  # Adjust as needed\n",
    "\n",
    "# Function to run the GAMS model\n",
    "def run_gams_model(gams_exec_path, model_file, gdx_file, output_dir):\n",
    "    # Construct the GAMS command\n",
    "    command = f'\"{gams_exec_path}\" \"{model_file}\" gdxin=\"{gdx_file}\" lo=2 gdx=\"{os.path.join(output_dir, \"output.gdx\")}\"'\n",
    "    # Execute the command\n",
    "    try:\n",
    "        subprocess.run(command, check=True, shell=True)\n",
    "        print(\"GAMS model executed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred in GAMS model execution: {e}\")\n",
    "\n",
    "# Export the database to a GDX file\n",
    "gdx_file_path = os.path.join(gams_output_dir, \"data.gdx\")\n",
    "try:\n",
    "    db.export(gdx_file_path)\n",
    "    print(f\"GDX file created successfully at: {gdx_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during GDX file creation: {e}\")\n",
    "\n",
    "# Run the GAMS model with the created GDX file\n",
    "run_gams_model(gams_executable_path, os.path.join(gams_output_dir, gams_model_file), gdx_file_path, gams_output_dir)\n",
    "\n",
    "# Add additional code here to retrieve and process results from GAMS, if necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve GAMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gams import *\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load the JSON data\n",
    "with open('data_for_gams_converted.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize a GAMS workspace\n",
    "ws = GamsWorkspace()\n",
    "\n",
    "# Create a new GAMS database\n",
    "db = ws.add_database()\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "VNFs = ['VNF1', 'VNF2', 'VNF3']\n",
    "Apps = ['App1', 'App2', 'App3']\n",
    "Servers = ['Server1', 'Server2', 'Server3']\n",
    "Users = ['User1', 'User2', 'User3']\n",
    "HostUtilizationCap = 100  # Example value\n",
    "\n",
    "# Adding sets to the database\n",
    "for set_name, elements in zip(['VNFs', 'Apps', 'Servers', 'Users'], [VNFs, Apps, Servers, Users]):\n",
    "    gams_set = db.add_set(set_name, 1)\n",
    "    for element in elements:\n",
    "        gams_set.add_record(element)\n",
    "\n",
    "# Adding HostUtilizationCap parameter\n",
    "host_utilization_param = db.add_parameter(\"HostUtilizationCap\", 1, \"Servers\")\n",
    "for server in Servers:\n",
    "    host_utilization_param.add_record(server).value = HostUtilizationCap\n",
    "\n",
    "# Define other parameters (replace with actual data)\n",
    "# Example structure for parameters (modify as per your data structure):\n",
    "Users_AP = {(user, server): 1 for user in Users for server in Servers}  # Example data\n",
    "Users_App = {(user, app): 1 for user in Users for app in Apps}  # Example data\n",
    "Topology = {(server1, server2): 1 for server1 in Servers for server2 in Servers}  # Example data\n",
    "LinkDelay = {(server1, server2): 10 for server1 in Servers for server2 in Servers}  # Example data\n",
    "App_Definition = {(vnf, app): 1 for vnf in VNFs for app in Apps}  # Example data\n",
    "BW_Req = {app: 50 for app in Apps}  # Example data\n",
    "CPU_Req = {vnf: 10 for vnf in VNFs}  # Example data\n",
    "LinkCap = {(server1, server2): 1000 for server1 in Servers for server2 in Servers}  # Example data\n",
    "UserBW = {user: 20 for user in Users}  # Example data\n",
    "UserSC = {(vnf, user): 1 for vnf in VNFs for user in Users}  # Example data\n",
    "SCs = {(user, vnf1, vnf2): 1 for user in Users for vnf1 in VNFs for vnf2 in VNFs}  # Example data\n",
    "\n",
    "# Add parameters to the database\n",
    "# This is a generalized way to add parameters, modify as needed for your specific data structure\n",
    "\n",
    "\n",
    "# Export the database to a GDX file\n",
    "gdx_file_path = \"C:\\\\Users\\\\youse\\\\Documents\\\\Github\\\\VNF-python\\\\output.gdx\"  # Change to your desired output path\n",
    "db.export(gdx_file_path)\n",
    "\n",
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def run_gams_model(gams_path, model_file, log_option, output_gdx):\n",
    "    \"\"\"\n",
    "    Run a GAMS model using Python.\n",
    "\n",
    "    Parameters:\n",
    "    gams_path (str): Full path to the GAMS executable.\n",
    "    model_file (str): The GAMS model file to run.\n",
    "    log_option (str): Log level option for GAMS.\n",
    "    input_gdx (str): GDX file for input data.\n",
    "    output_gdx (str): GDX file for output data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the command to run the GAMS model\n",
    "    command = f'\"{gams_path}\" {model_file} lo={log_option} gdx={output_gdx}'\n",
    "\n",
    "    # Run the command and capture output\n",
    "    try:\n",
    "        completed_process = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(\"GAMS model executed successfully.\")\n",
    "        print(completed_process.stdout.decode())\n",
    "        print(completed_process.stderr.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred in GAMS model execution.\")\n",
    "        print(e)\n",
    "        print(e.stdout.decode())\n",
    "        print(e.stderr.decode())\n",
    "\n",
    "# Path to your GAMS installation\n",
    "gams_path = 'C:\\\\GAMS\\\\win64\\\\24.9\\\\gams'\n",
    "\n",
    "# Your GAMS model file\n",
    "model_file = 'm2'\n",
    "\n",
    "# Define the path to your input GDX file\n",
    "input_gdx = 'data.gdx'\n",
    "\n",
    "# Log option and output GDX file name\n",
    "log_option = '3'\n",
    "output_gdx = 'Optimization_output'\n",
    "\n",
    "# Run the GAMS model\n",
    "run_gams_model(gams_path, model_file, log_option, output_gdx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_gams_model(gams_executable_path, model_file_path, log_option, input_gdx_path, output_gdx_path):\n",
    "    \"\"\"\n",
    "    Run a GAMS model using Python through subprocess.\n",
    "\n",
    "    Args:\n",
    "    gams_executable_path (str): Full path to the GAMS executable.\n",
    "    model_file_path (str): Full path to the GAMS model file (.gms) to run.\n",
    "    log_option (str): GAMS log option for verbosity (e.g., '3' for detailed log).\n",
    "    input_gdx_path (str): Full path to the input GDX file.\n",
    "    output_gdx_path (str): Full path where the output GDX file will be saved.\n",
    "    \"\"\"\n",
    "    # Construct the command string to run the GAMS model\n",
    "    command = f'\"{gams_executable_path}\" \"{model_file_path}\" lo={log_option} gdx=\"{output_gdx_path}\" gdxin=\"{input_gdx_path}\"'\n",
    "    \n",
    "    try:\n",
    "        # Execute the command\n",
    "        completed_process = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "        # Print the standard output from GAMS\n",
    "        print(\"GAMS model executed successfully:\\n\", completed_process.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Print any errors encountered during GAMS execution\n",
    "        print(\"An error occurred in GAMS model execution:\\n\", e.stderr.decode())\n",
    "\n",
    "# Path to the GAMS executable\n",
    "gams_executable_path = 'C:\\\\GAMS\\\\win64\\\\24.9\\\\gams.exe'\n",
    "\n",
    "# Full path to your GAMS model file\n",
    "model_file_path = 'C:\\\\Users\\\\youse\\\\Documents\\\\Github\\\\VNF-python\\\\m2.gms'\n",
    "\n",
    "# Log option for GAMS\n",
    "log_option = '3'\n",
    "\n",
    "# Full path to your input GDX file\n",
    "input_gdx_path = 'C:\\\\Users\\\\youse\\\\Documents\\\\Github\\\\VNF-python\\\\data.gdx'\n",
    "\n",
    "# Full path where the output GDX file should be saved\n",
    "output_gdx_path = 'C:\\\\Users\\\\youse\\\\Documents\\\\Github\\\\VNF-python\\\\Optimization_output.gdx'\n",
    "\n",
    "# Run the GAMS model with specified parameters\n",
    "run_gams_model(gams_executable_path, model_file_path, log_option, input_gdx_path, output_gdx_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print variables that have been read so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "def write_to_file(file_path, data):\n",
    "    with open(file_path, 'a') as file:  # Append mode\n",
    "        file.write(data + \"\\n\")\n",
    "\n",
    "\n",
    "def print_and_save_servers(G, file_path):\n",
    "    table = PrettyTable([\"Server\", \"CPU\", \"Memory\", \"Storage\"])\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        cpu = f\"{attrs.get('cpu', 'N/A')} GHz\"\n",
    "        memory = f\"{attrs.get('memory', 'N/A')} GB\"\n",
    "        storage = f\"{attrs.get('storage', 'N/A')} TB\"\n",
    "        table.add_row([node, cpu, memory, storage])\n",
    "    print(\"Servers and their capacities:\\n\" + table.get_string())\n",
    "    write_to_file(file_path, \"Servers and their capacities:\\n\" + table.get_string())\n",
    "\n",
    "def print_and_save_network_graph_with_index(G, file_path):\n",
    "    table = PrettyTable([\"#\", \"Link\", \"Capacity\", \"Cost\", \"Latency\"])\n",
    "    for index, (u, v, attrs) in enumerate(G.edges(data=True), start=1):\n",
    "        table.add_row([index, f\"{u}-{v}\", attrs.get('capacity', 'N/A'), attrs.get('cost', 'N/A'), attrs.get('latency', 'N/A')])\n",
    "    print(\"\\nNetwork Graph (Servers and Links):\\n\" + table.get_string())\n",
    "    write_to_file(file_path, \"\\nNetwork Graph (Servers and Links):\\n\" + table.get_string())\n",
    "\n",
    "def print_and_save_user_graph(G_users, file_path):\n",
    "    table = PrettyTable([\"User\", \"Connected Node\", \"Application\", \"Bandwidth\", \"Latency\"])\n",
    "    for user, attrs in G_users.nodes(data=True):\n",
    "        table.add_row([user, attrs.get('associated_ap', 'N/A'), attrs.get('application', 'N/A'), attrs.get('bandwidth', 'N/A'), attrs.get('latency', 'N/A')])\n",
    "    print(\"\\nUser Graph:\\n\" + table.get_string())\n",
    "    write_to_file(file_path, \"\\nUser Graph:\\n\" + table.get_string())\n",
    "\n",
    "def print_and_save_users_and_apps(app_to_users, user_to_node_map, file_path):\n",
    "    table = PrettyTable([\"Application\", \"User\", \"Connected Node\"])\n",
    "    for app, users in app_to_users.items():\n",
    "        for user in users:\n",
    "            table.add_row([app, user, user_to_node_map.get(user, 'N/A')])\n",
    "    print(\"\\nUsers and Applications:\\n\" + table.get_string())\n",
    "    write_to_file(file_path, \"\\nUsers and Applications:\\n\" + table.get_string())\n",
    "\n",
    "def print_and_save_application_to_cnfs_mapping(containers_data, file_path):\n",
    "    for application in containers_data.get('Containers', []):\n",
    "        app_name = application.get('application', 'Unknown Application')\n",
    "        cnfs_table = PrettyTable([\"Application\", \"Type\", \"Name\", \"CPU (Cores)\", \"Memory (GB)\", \"Storage (GB)\"])\n",
    "        for cnf in application.get('vnfs', []):  # Network Functions\n",
    "            cnfs_table.add_row([app_name, \"Network Function\", cnf['name'], cnf['cpu'], cnf['memory'], cnf['storage']])\n",
    "        for analytics_cnf in application.get('microservices', []):  # Analytics Functions\n",
    "            cnfs_table.add_row([app_name, \"Analytics Function\", analytics_cnf['name'], analytics_cnf['cpu'], analytics_cnf['memory'], analytics_cnf['storage']])\n",
    "        print(cnfs_table.get_string())\n",
    "        write_to_file(file_path, cnfs_table.get_string() + \"\\n\")\n",
    "\n",
    "# Define a function to orchestrate all printing and saving operations\n",
    "def orchestrate_all(file_path):\n",
    "    # Ensure the file is cleared before writing\n",
    "    open(file_path, 'w').close()\n",
    "    \n",
    "    # Call each function with the file path to append data\n",
    "    print_and_save_servers(G, file_path)\n",
    "    print_and_save_network_graph_with_index(G, file_path)\n",
    "    print_and_save_user_graph(G_users, file_path)\n",
    "    print_and_save_users_and_apps(app_to_users, user_to_node_map, file_path)\n",
    "    print_and_save_application_to_cnfs_mapping(containers_data, file_path)\n",
    "\n",
    "# Specify the path for the output file\n",
    "#output_file_path = \"complete_network_mapping.txt\"\n",
    "\n",
    "# Call the orchestrating function\n",
    "#orchestrate_all(output_file_path)\n",
    "\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def generate_comprehensive_table(containers_data, app_to_users):\n",
    "    table = PrettyTable([\"App ID\", \"Application\", \"CNF Type\", \"CNF Name\", \"CPU (Cores)\", \"Memory (GB)\", \"Storage (GB)\", \"Associated Users\"])\n",
    "    \n",
    "    app_id = 1  # Initialize application ID counter\n",
    "    for container in containers_data['Containers']:\n",
    "        app_name = container['application']\n",
    "        # Gather all users associated with this application\n",
    "        users = ', '.join(app_to_users.get(app_name, []))\n",
    "        \n",
    "        # Process each CNF (Network Function and Analytics Function)\n",
    "        for cnf in container.get('vnfs', []) + container.get('microservices', []):\n",
    "            cnf_type = \"Network Function\" if cnf in container.get('vnfs', []) else \"Analytics Function\"\n",
    "            table.add_row([app_id, app_name, cnf_type, cnf['name'], cnf['cpu'], cnf['memory'], cnf['storage'], users])\n",
    "        \n",
    "        app_id += 1  # Increment application ID for the next application\n",
    "    \n",
    "    print(table)\n",
    "    # Save the table to a file\n",
    "    with open(\"comprehensive_network_mapping.txt\", \"w\") as f:\n",
    "        f.write(table.get_string())\n",
    "\n",
    "# Assuming `containers_data` and `app_to_users` are defined\n",
    "#generate_comprehensive_table(containers_data, app_to_users)\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def generate_sorted_cnf_user_summary(containers_data, app_to_users):\n",
    "    # Initialize dictionaries to hold CNF details and associated user count for each type\n",
    "    network_functions = {}\n",
    "    analytics_functions = {}\n",
    "\n",
    "    # Loop through each container (application) to gather CNF and user details\n",
    "    for container in containers_data:\n",
    "        app_name = container['application']\n",
    "        users = app_to_users.get(app_name, [])  # Get users associated with this application\n",
    "        \n",
    "        # Process both Network Functions (vnfs) and Analytics Functions (microservices)\n",
    "        for cnf_type, cnf_list in [('Network Function', container.get('vnfs', [])), \n",
    "                                   ('Analytics Function', container.get('microservices', []))]:\n",
    "            for cnf in cnf_list:\n",
    "                cnf_name = cnf['name']\n",
    "                # Determine which dictionary to use based on the CNF type\n",
    "                if cnf_type == 'Network Function':\n",
    "                    cnf_dict = network_functions\n",
    "                else:\n",
    "                    cnf_dict = analytics_functions\n",
    "                \n",
    "                # Key to uniquely identify CNFs by name and type\n",
    "                cnf_key = (cnf_name, cnf_type)\n",
    "                \n",
    "                # Initialize or update the CNF entry in the dictionary\n",
    "                if cnf_key not in cnf_dict:\n",
    "                    cnf_dict[cnf_key] = {'applications': set(), 'users': set()}\n",
    "                cnf_dict[cnf_key]['applications'].add(app_name)\n",
    "                cnf_dict[cnf_key]['users'].update(users)\n",
    "    \n",
    "    # Sort the network functions and analytics functions separately\n",
    "    sorted_network_functions = sorted(network_functions.items(), key=lambda item: len(item[1]['users']), reverse=True)\n",
    "    sorted_analytics_functions = sorted(analytics_functions.items(), key=lambda item: len(item[1]['users']), reverse=True)\n",
    "    \n",
    "    # Merge the sorted lists into a single list\n",
    "    sorted_cnfs = sorted_network_functions + sorted_analytics_functions\n",
    "    \n",
    "    # Create the summary table\n",
    "    summary_table = PrettyTable([\"CNF ID\", \"CNF Name\", \"CNF Type\", \"Applications\", \"Associated Users\", \"User Count\"])\n",
    "    cnf_id = 1\n",
    "    for (cnf_name, cnf_type), details in sorted_cnfs:\n",
    "        applications_str = ', '.join(sorted(details['applications']))\n",
    "        users_str = ', '.join(sorted([str(user.split('_')[-1]) for user in details['users']]))\n",
    "        user_count = len(details['users'])\n",
    "        summary_table.add_row([cnf_id, cnf_name, cnf_type, applications_str, users_str, user_count])\n",
    "        cnf_id += 1\n",
    "    \n",
    "    return summary_table\n",
    "\n",
    "def write_summary_to_file(summary_table, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(str(summary_table))\n",
    "\n",
    "# Assuming `containers_data` and `app_to_users` are correctly populated\n",
    "# Correctly call the function with your actual data\n",
    "summary_table = generate_sorted_cnf_user_summary(containers_data, app_to_users)\n",
    "write_summary_to_file(summary_table, \"sorted_cnf_user_summary.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test GAMS with Classes Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Configuration Classes and Orchestration\n",
    "\n",
    "This documentation covers the implementation and usage of a set of Python classes designed to model a network's configuration and an orchestration function to aggregate and output this configuration. These tools are ideal for representing and managing complex network topologies within simulation environments or for documentation purposes.\n",
    "\n",
    "\n",
    "**Base Classes**\n",
    "* NetworkResource: Represents basic computational resources (CPU, memory, storage) that can be associated with various network entities.\n",
    "\n",
    "* NetworkEntity: An abstract base class for all network entities. It supports resource updates and serves as the superclass for more specific entity types.\n",
    "\n",
    "**Entity Classes**\n",
    "- Server: Models a server within the network, holding resources and identifiable by a unique ID.\n",
    "\n",
    "- Link: Represents a network link between two points, characterized by its source, destination, capacity, cost, and latency.\n",
    "\n",
    "- User: Describes a network user, including their associated access point, application usage, bandwidth, and latency requirements.\n",
    "\n",
    "- CNF (Container Network Function): Defines a containerized network function or service, including its type and resource requirements.\n",
    "\n",
    "- Application: Encapsulates an application, potentially comprising multiple CNFs, and supports dynamic CNF association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "# Base class for network resources\n",
    "class NetworkResource:\n",
    "    def __init__(self, cpu=0, memory=0, storage=0):\n",
    "        self.cpu = cpu\n",
    "        self.memory = memory\n",
    "        self.storage = storage\n",
    "\n",
    "# Base class for all network entities\n",
    "class NetworkEntity:\n",
    "    def __init__(self, id, resources=None):\n",
    "        self.id = id\n",
    "        self.resources = resources or NetworkResource()\n",
    "\n",
    "    def update_resources(self, cpu_delta=0, memory_delta=0, storage_delta=0):\n",
    "        self.resources.cpu += cpu_delta\n",
    "        self.resources.memory += memory_delta\n",
    "        self.resources.storage += storage_delta\n",
    "\n",
    "class Server(NetworkEntity):\n",
    "    def __init__(self, id, resources):\n",
    "        super().__init__(id, resources)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_and_save(G, file_path, status=None):\n",
    "        table = PrettyTable([\"ID\", \"CPU (GHz)\", \"Memory (GB)\", \"Storage (TB)\"])\n",
    "        for node, attrs in G.nodes(data=True):\n",
    "            if attrs.get('type', '') == 'server':\n",
    "                resources = attrs['resources']\n",
    "                table.add_row([node, resources.cpu, resources.memory, resources.storage])\n",
    "        print(table)\n",
    "        write_to_file(file_path, table.get_string(title=\"Servers\"))\n",
    "\n",
    "class Link:\n",
    "    @staticmethod\n",
    "    def print_and_save(G, file_path):\n",
    "        table = PrettyTable([\"Source\", \"Destination\", \"Capacity\", \"Cost\", \"Latency\"])\n",
    "        for u, v, attrs in G.edges(data=True):\n",
    "            table.add_row([u, v, attrs.get('capacity', 'N/A'), attrs.get('cost', 'N/A'), attrs.get('latency', 'N/A')])\n",
    "        print(table)\n",
    "        write_to_file(file_path, table.get_string(title=\"Links\"))\n",
    "\n",
    "class User:\n",
    "    @staticmethod\n",
    "    def print_and_save(G_users, file_path):\n",
    "        table = PrettyTable([\"User\", \"Associated AP\", \"Application\", \"Bandwidth\", \"Latency\"])\n",
    "        for user, attrs in G_users.nodes(data=True):\n",
    "            table.add_row([user, attrs.get('associated_ap', 'N/A'), attrs.get('application', 'N/A'), attrs.get('bandwidth', 'N/A'), attrs.get('latency', 'N/A')])\n",
    "        print(table)\n",
    "        write_to_file(file_path, table.get_string(title=\"Users\"))\n",
    "\n",
    "class CNF(NetworkEntity):\n",
    "    def __init__(self, id, resources, cnf_type='NF'):\n",
    "        super().__init__(id, resources)\n",
    "        self.cnf_type = cnf_type\n",
    "\n",
    "class Application(NetworkEntity):\n",
    "    def __init__(self, id, resources=None, sfc_id=None):\n",
    "        super().__init__(id, resources)\n",
    "        self.sfc_id = sfc_id\n",
    "        self.cnfs = []  # Assuming CNFs are added as objects of class CNF or its children\n",
    "\n",
    "    # Method to add a CNF to the application\n",
    "    def add_cnf(self, cnf):\n",
    "        self.cnfs.append(cnf)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_and_save_applications(containers_data, file_path):\n",
    "        # Check if containers_data is a list of applications directly\n",
    "        if isinstance(containers_data, list) and all(isinstance(app, dict) for app in containers_data):\n",
    "            for app_data in containers_data:\n",
    "                # Each app_data is a dict, potentially representing an Application object\n",
    "                table = PrettyTable([\"Application ID\", \"Type\", \"Name\", \"CPU (Cores)\", \"Memory (GB)\", \"Storage (GB)\"])\n",
    "                # Assuming 'vnfs' and 'microservices' are keys in each app_data dict\n",
    "                cnfs = app_data.get('vnfs', []) + app_data.get('microservices', [])\n",
    "                for cnf in cnfs:\n",
    "                    # Assuming cnf is a dict with keys 'name', 'cpu', 'memory', 'storage'\n",
    "                    table.add_row([app_data.get('application'), cnf.get('type', 'N/A'), cnf['name'], cnf.get('cpu', 'N/A'), cnf.get('memory', 'N/A'), cnf.get('storage', 'N/A')])\n",
    "                print(table)\n",
    "                write_to_file(file_path, table.get_string(title=f\"Application: {app_data.get('application')}\"))\n",
    "        else:\n",
    "            print(\"Invalid containers_data format. Expected a list of dictionaries representing applications.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "- write_to_file: A helper function to append data to a specified file, ensuring readability with added newlines.\n",
    "- orchestrate_all: An orchestration function that aggregates information from network entities (servers, links, users, applications) and outputs this data, both printed and saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Helper function to write data to a file\n",
    "def write_to_file(file_path, data):\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(data + \"\\n\\n\")  # Add extra newlines for readability\n",
    "\n",
    "# Orchestration function to call the print and save functions for all entities\n",
    "def orchestrate_all(G, G_users, containers_data, file_path):\n",
    "    # Clear the file before starting\n",
    "    open(file_path, 'w').close()\n",
    "    \n",
    "    # Print and save servers, links, users, applications, and CNFs\n",
    "    Server.print_and_save(G, file_path)\n",
    "    Link.print_and_save(G, file_path)\n",
    "    User.print_and_save(G_users, file_path)\n",
    "    Application.print_and_save_applications(containers_data, file_path)\n",
    "\n",
    "# Assuming G, G_users, and containers_data are defined elsewhere\n",
    "file_path = \"network_details.txt\"\n",
    "orchestrate_all(G, G_users, containers_data, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets connect with GAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Define paths\n",
    "workspace_dir = os.getcwd()\n",
    "gdx_file_path = os.path.join(workspace_dir, 'input.gdx')\n",
    "\n",
    "# Initialize GAMS workspace\n",
    "ws = GamsWorkspace(working_directory=workspace_dir)\n",
    "\n",
    "# Create a new GDX file\n",
    "db = ws.add_database()\n",
    "\n",
    "# Add dummy data\n",
    "# Creating a set named 'nodes' and a parameter 'capacity'\n",
    "nodes = db.add_set(\"nodes\", 1, \"Network nodes\")\n",
    "nodes.add_record(\"Node1\")\n",
    "nodes.add_record(\"Node2\")\n",
    "\n",
    "capacity = db.add_parameter(\"capacity\", 1, \"Capacity of nodes\")\n",
    "capacity.add_record(\"Node1\").value = 100\n",
    "capacity.add_record(\"Node2\").value = 200\n",
    "\n",
    "# Export the database to GDX\n",
    "db.export(gdx_file_path)\n",
    "print(f\"Created GDX file with dummy data at {gdx_file_path}\")\n",
    "\n",
    "# Load the GDX file\n",
    "db_imported = ws.add_database_from_gdx(gdx_file_path)\n",
    "print(\"Contents of the GDX file:\")\n",
    "\n",
    "# Iterate over all symbols in the database\n",
    "for symbol in db_imported:\n",
    "    symbol_type = symbol.__class__.__name__\n",
    "    # Check if the symbol is a Set\n",
    "    if symbol_type == \"GamsSet\":\n",
    "        print(f\"Set: {symbol.name}\")\n",
    "        for rec in symbol:\n",
    "            print(f\"  - {rec.keys[0]}\")\n",
    "    # Check if the symbol is a Parameter\n",
    "    elif symbol_type == \"GamsParameter\":\n",
    "        print(f\"Parameter: {symbol.name}\")\n",
    "        for rec in symbol:\n",
    "            print(f\"  - {rec.keys[0]}: {rec.value}\")\n",
    "    # Add more checks as necessary for other types like GamsVariable, GamsEquation, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GAMS connection with dummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Explanation:\n",
    "\n",
    "The code initializes the GAMS workspace and defines the paths for input and output GDX files. It ensures that the GAMS Python API is accessible by adjusting the system path to include the directory where the API files are located (gams_api_dir).\n",
    "It creates a new GDX file (input.gdx) with dummy data representing network nodes and their capacities. This is achieved by utilizing the GAMS Python API to add sets and parameters to a GAMS database (db) and exporting it to a GDX file.\n",
    "The dummy data is loaded into the GAMS workspace from the input GDX file, and its contents are printed. This demonstrates how the GAMS Python API allows for seamless interaction with GAMS databases and GDX files, enabling easy data exchange between Python and GAMS.\n",
    "The GAMS model (test_model.gms) is executed using the input GDX file as input, and the results are stored in output.gdx. This is done by running the GAMS model from the command line using the os.system() function, passing the input and output GDX file paths as arguments.\n",
    "The output GDX file is read, and its contents are printed. The GAMS Python API is again utilized to load the output GDX file into the GAMS workspace and iterate over its symbols to access and display the data.\n",
    "Additional Details:\n",
    "\n",
    "Paths: The paths for the input and output GDX files are defined explicitly, ensuring clarity and transparency. The gams_api_dir variable contains the directory where the GAMS Python API files are located.\n",
    "Libraries: The GAMS Python API is imported from the directory specified by gams_api_dir, which contains the necessary files for interfacing with GAMS from Python.\n",
    "Python Version: The code is designed to work with Python 3.6.8, ensuring compatibility with the specific version of Python being used.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Adjust the path to where your GAMS Python API is located\n",
    "gams_api_dir = \"C:\\\\GAMS\\\\win64\\\\24.9\\\\apifiles\\\\Python\\\\api_36\\\\\"\n",
    "sys.path.append(gams_api_dir)\n",
    "\n",
    "#update system path then import gams libraries\n",
    "from gams.workspace import GamsWorkspace\n",
    "\n",
    "\n",
    "# Define paths\n",
    "workspace_dir = os.getcwd()\n",
    "input_gdx_path = os.path.join(workspace_dir, 'input.gdx')\n",
    "output_gdx_path = os.path.join(workspace_dir, 'output.gdx')\n",
    "\n",
    "# Initialize GAMS workspace\n",
    "ws = GamsWorkspace(working_directory=workspace_dir)\n",
    "\n",
    "# Create a new GDX file with dummy data\n",
    "db = ws.add_database()\n",
    "nodes = db.add_set(\"nodes\", 1, \"Network nodes\")\n",
    "nodes.add_record(\"Node1\")\n",
    "nodes.add_record(\"Node2\")\n",
    "capacity = db.add_parameter(\"capacity\", 1, \"Capacity of nodes\")\n",
    "capacity.add_record(\"Node1\").value = 100\n",
    "capacity.add_record(\"Node2\").value = 200\n",
    "db.export(input_gdx_path)\n",
    "print(f\"Created input GDX file with dummy data at {input_gdx_path}\")\n",
    "\n",
    "# Load the GDX file\n",
    "db_imported = ws.add_database_from_gdx(input_gdx_path)\n",
    "print(\"Contents of the input GDX file:\")\n",
    "for symbol in db_imported:\n",
    "    symbol_type = symbol.__class__.__name__\n",
    "    if symbol_type == \"GamsSet\":\n",
    "        print(f\"Set: {symbol.name}\")\n",
    "        for rec in symbol:\n",
    "            print(f\"  - {rec.keys[0]}\")\n",
    "    elif symbol_type == \"GamsParameter\":\n",
    "        print(f\"Parameter: {symbol.name}\")\n",
    "        for rec in symbol:\n",
    "            print(f\"  - {rec.keys[0]}: {rec.value}\")\n",
    "\n",
    "# Solve the GAMS model\n",
    "model_file = 'test_model.gms'\n",
    "command = f'gams {model_file} --input=\"{input_gdx_path}\" --output=\"{output_gdx_path}\"'\n",
    "print(f\"Running GAMS model: {command}\")\n",
    "os.system(command)\n",
    "\n",
    "# Read the output GDX file\n",
    "db_output = ws.add_database_from_gdx(output_gdx_path)\n",
    "print(\"\\nContents of the output GDX file:\")\n",
    "for symbol in db_output:\n",
    "    symbol_type = symbol.__class__.__name__\n",
    "    if symbol_type == \"GamsSet\":\n",
    "        print(f\"Set: {symbol.name}\")\n",
    "        for rec in symbol:\n",
    "            print(f\"  - {rec.keys[0]}\")\n",
    "    elif symbol_type == \"GamsParameter\":\n",
    "        print(f\"Parameter: {symbol.name}\")\n",
    "        for rec in symbol:\n",
    "            print(f\"  - {rec.keys[0]}: {rec.value}\")\n",
    "\n",
    "\n",
    "'''GAMS MODEL:\n",
    "\n",
    "$ontext\n",
    "This GAMS file reads the input GDX file, solves the model, and creates the output GDX file.\n",
    "$offtext\n",
    "\n",
    "set nodes / Node1, Node2 /;\n",
    "parameter capacity(nodes);\n",
    "\n",
    "$ontext\n",
    "Read input GDX file\n",
    "$offtext\n",
    "\n",
    "$gdxin input.gdx\n",
    "$load nodes capacity\n",
    "$gdxin\n",
    "\n",
    "display capacity;\n",
    "\n",
    "$ontext\n",
    "Define parameters for the LP model\n",
    "$offtext\n",
    "\n",
    "parameter a / 2 /;\n",
    "parameter b / 3 /;\n",
    "\n",
    "$ontext\n",
    "Define LP model\n",
    "$offtext\n",
    "\n",
    "* Define variables\n",
    "variables\n",
    "    x(nodes);\n",
    "\n",
    "* Define equations\n",
    "equations\n",
    "    objective;\n",
    "\n",
    "* Define objective function\n",
    "objective..\n",
    "    z =e= a * b * sum(nodes, x(nodes));\n",
    "\n",
    "* Define constraints (if any)\n",
    "*constraints\n",
    "*    constraint1..\n",
    "*        sum(nodes, x(nodes)) =l= 10;\n",
    "\n",
    "* Solve LP model\n",
    "* Model type: LP (Linear Programming)\n",
    "model lp_model /all/;\n",
    "solve lp_model using lp maximizing z;\n",
    "\n",
    "$ontext\n",
    "Create output GDX file\n",
    "$offtext\n",
    "\n",
    "set nodes / Node1, Node2 /;\n",
    "parameter solution(nodes);\n",
    "\n",
    "* Read solved model results\n",
    "solution(nodes) = x.l;\n",
    "\n",
    "* Export results to output GDX file\n",
    "$gdxout output.gdx\n",
    "$unload solution\n",
    "$gdxout\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GDX Actual Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script demonstrates creating, running, and processing GAMS models dynamically with Python. It's designed with modularity and maintainability in mind, following best coding practices. This approach includes comprehensive documentation, dynamic input handling for GAMS database creation, and test-driven development strategies.\n",
    "\n",
    "Highlights:\n",
    "- Modular code structure for straightforward maintenance and future extensions.\n",
    "- Clear documentation using docstrings.\n",
    "- Dynamic input handling to populate GAMS sets and parameters.\n",
    "- A test function for code validation and hardening.\n",
    "\n",
    "Note:\n",
    "- Adjust the GAMS_API_DIR variable to match the GAMS installation's API directory on your system. This step is crucial for seamless Python-GAMS interaction.\n",
    "- Ensure to append the GAMS API directory to sys.path before importing the GAMS module to prevent ModuleNotFoundError.\n",
    "- The GAMS model (`model.gms`) should be properly set up to read from `input.gdx` and write to `output.gdx`. Adjust the file paths as necessary.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration for GAMS Python API location\n",
    "GAMS_API_DIR = \"C:\\\\GAMS\\\\win64\\\\24.9\\\\apifiles\\\\Python\\\\api_36\\\\\"\n",
    "sys.path.append(GAMS_API_DIR)\n",
    "\n",
    "\n",
    "from gams.workspace import GamsWorkspace, GamsException, DebugLevel  # Add DebugLevel here\n",
    "\n",
    "\n",
    "\n",
    "def initialize_workspace():\n",
    "    \"\"\"Initializes and returns a GAMS Workspace object.\"\"\"\n",
    "    return GamsWorkspace()\n",
    "\n",
    "def create_database(ws):\n",
    "    \"\"\"Creates and returns a GAMS database within the given workspace.\"\"\"\n",
    "    return ws.add_database()\n",
    "\n",
    "def populate_set(db, set_name, elements):\n",
    "    \"\"\"Populates a GAMS set with specified elements.\"\"\"\n",
    "    gams_set = db.add_set(set_name, 1, \"Dynamic set\")\n",
    "    for element in elements:\n",
    "        gams_set.add_record(element)\n",
    "\n",
    "def export_database(db, file_name=\"input.gdx\"):\n",
    "    \"\"\"Exports the GAMS database to a GDX file.\"\"\"\n",
    "    db.export(file_name)\n",
    "    print(f\"Database successfully exported to '{file_name}'.\")\n",
    "\n",
    "\n",
    "def run_gams_model(ws, model_file=\"model.gms\", input_gdx=\"input.gdx\", output_gdx=\"output.gdx\"):\n",
    "    \"\"\"\n",
    "    Runs a specified GAMS model, specifying input and output GDX files.\n",
    "    Includes enhanced debugging for detailed diagnostics.\n",
    "    \"\"\"\n",
    "    # Ensure the model file exists\n",
    "    if not os.path.isfile(model_file):\n",
    "        print(f\"Model file '{model_file}' not found.\")\n",
    "        return\n",
    "    \n",
    "    job = ws.add_job_from_file(model_file)\n",
    "    opt = ws.add_options()\n",
    "    opt.defines[\"gdxin\"] = input_gdx\n",
    "    opt.defines[\"gdxout\"] = output_gdx\n",
    "    \n",
    "    # Print debug information\n",
    "    print(f\"Running GAMS model with input '{input_gdx}' and output '{output_gdx}'\")\n",
    "    \n",
    "    try:\n",
    "        job.run(opt)\n",
    "        print(f\"GAMS model '{model_file}' executed successfully. Results in '{output_gdx}'.\")\n",
    "    except GamsException as e:\n",
    "        print(f\"Error executing GAMS model: {e}\")\n",
    "\n",
    "\n",
    "def populate_cost_parameter(db, set_name, cost_values):\n",
    "    \"\"\"\n",
    "    Populates the cost parameter for each element in the specified set.\n",
    "    \n",
    "    Parameters:\n",
    "    - db: GamsDatabase, the database to which the parameter is added.\n",
    "    - set_name: str, the name of the set corresponding to the cost parameter.\n",
    "    - cost_values: dict, a dictionary mapping each set element to its cost.\n",
    "    \"\"\"\n",
    "    cost_param = db.add_parameter(\"cost\", 1, \"Cost parameter\")\n",
    "    for element, value in cost_values.items():\n",
    "        cost_param.add_record(element).value = value\n",
    "\n",
    "\n",
    "\n",
    "def read_and_print_output_gdx(file_name=\"output.gdx\"):\n",
    "    \"\"\"Reads and prints contents of a GDX file in a tabular format if it exists.\"\"\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"File {file_name} does not exist. Skipping.\")\n",
    "        return\n",
    "    ws = GamsWorkspace()\n",
    "    db = ws.add_database_from_gdx(file_name)\n",
    "    for symbol in db:\n",
    "        print(f\"\\n{symbol.name}:\")\n",
    "        for rec in symbol:\n",
    "            print(f\"{rec.keys} : {rec.value}\")\n",
    "\n",
    "\n",
    "def process_container_data(containers_data):\n",
    "    \"\"\"Extracts unique service names from container data.\"\"\"\n",
    "    services = set()\n",
    "    for container in containers_data:\n",
    "        if not isinstance(container, dict):\n",
    "            raise ValueError(\"Expected a dictionary for each container, got: {}\".format(type(container)))\n",
    "        services.update(container.get('sfc', []))\n",
    "    return services\n",
    "\n",
    "\n",
    "\n",
    "# Correct the GAMS system directory to point to the root of the GAMS installation\n",
    "GAMS_SYSTEM_DIR = \"C:\\\\GAMS\\\\win64\\\\24.9\"  # Adjust this path as necessary to match your GAMS installation\n",
    "\n",
    "\n",
    "def main(containers_data):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate GDX file creation, GAMS model execution, and output processing.\n",
    "    \"\"\"\n",
    "    # Initialize the GAMS workspace with the system directory\n",
    "    #ws = GamsWorkspace(system_directory=GAMS_SYSTEM_DIR)\n",
    "    ws = GamsWorkspace(system_directory=GAMS_SYSTEM_DIR, debug=DebugLevel.KeepFiles)\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"GAMS System Directory:\", ws.system_directory)\n",
    "    db = create_database(ws)\n",
    "\n",
    "    # Populate the database and export to GDX\n",
    "    unique_services = process_container_data(containers_data)\n",
    "    populate_set(db, \"CNFNameSet\", unique_services)\n",
    "    export_database(db, \"input.gdx\")\n",
    "\n",
    "    # Run the GAMS model\n",
    "    run_gams_model(ws, \"model.gms\", \"input.gdx\", \"output.gdx\")\n",
    "\n",
    "    # Read and print output from GDX\n",
    "    read_and_print_output_gdx(\"output.gdx\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(containers_data)\n",
    "#     test_process_container_data()\n",
    "#     main(containers_data)\n",
    "\n",
    "\n",
    "\n",
    "# Test function to validate process_container_data functionality\n",
    "def test_process_container_data():\n",
    "    test_data = [\n",
    "        {'application': 'TestApp1', 'sfc': ['TestService1', 'TestService2']},\n",
    "        {'application': 'TestApp2', 'sfc': ['TestService2', 'TestService3']}\n",
    "    ]\n",
    "    expected = {'TestService1', 'TestService2', 'TestService3'}\n",
    "    assert process_container_data(test_data) == expected, \"Test failed.\"\n",
    "    print(\"Test passed successfully.\")\n",
    "\n",
    "# Uncomment to run the test function\n",
    "\n",
    "\n",
    "    # Initialize the workspace and database\n",
    "    db = create_database(ws)\n",
    "\n",
    "    # Process the container data to extract unique service names and other parameters\n",
    "    services = process_container_data(containers_data)\n",
    "    populate_set(db, \"services\", services)\n",
    "\n",
    "    # Here, you would also populate other sets and parameters as required by your model, for example:\n",
    "    # Note: You need to adapt the following lines according to your actual data structure and model needs.\n",
    "    bandwidths = {data['application']: data['bandwidth'] for data in containers_data}\n",
    "    latencies = {data['application']: data['latency'] for data in containers_data}\n",
    "    device_densities = {data['application']: data['device_density'] for data in containers_data}\n",
    "    \n",
    "    # Populate these parameters into the database\n",
    "    populate_cost_parameter(db, \"bandwidths\", bandwidths)\n",
    "    populate_cost_parameter(db, \"latencies\", latencies)\n",
    "    populate_cost_parameter(db, \"device_densities\", device_densities)\n",
    "\n",
    "    # Export the database to a GDX file for use by the GAMS model\n",
    "    export_database(db)\n",
    "\n",
    "    # Run the GAMS model\n",
    "    run_gams_model(ws)\n",
    "\n",
    "    # After running the model, read and print the output from the GDX file\n",
    "    read_and_print_output_gdx()\n",
    "    \n",
    "    # Additional processing can be added here based on the GAMS model output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming containers_data is defined outside this script, pass it to the main function\n",
    "    print(\"containers_data contents:\")\n",
    "    print(containers_data)\n",
    "\n",
    "\n",
    "    print(\"G contents:\")\n",
    "    print(G)\n",
    "    \n",
    "    print(\"G_users contents:\")\n",
    "    print(G_users)\n",
    "\n",
    "    print(\"app_to_users contents:\")\n",
    "    print(app_to_users)\n",
    "\n",
    "    print(\"user_to_node_map contents:\")\n",
    "    print(user_to_node_map)\n",
    "\n",
    "    print(\"G.nodes contents:\")\n",
    "    print(G.nodes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "# Configuration for GAMS Python API location\n",
    "GAMS_API_DIR = \"C:\\\\GAMS\\\\win64\\\\24.9\\\\apifiles\\\\Python\\\\api_36\\\\\"\n",
    "sys.path.append(GAMS_API_DIR)\n",
    "\n",
    "from gams import *\n",
    "\n",
    "def create_gdx_file(G, G_users, containers_data, gdx_file_path):\n",
    "    # Initialize a GAMSDatabase\n",
    "    db = GAMSDatabase()\n",
    "\n",
    "    # Define sets\n",
    "    servers_set = set()\n",
    "    links_set = set()\n",
    "    users_set = set()\n",
    "    applications_set = set()\n",
    "\n",
    "    # Populate sets with data from the NetworkX graph and other sources\n",
    "    for node in G.nodes():\n",
    "        if 'type' in G.nodes[node]:\n",
    "            if G.nodes[node]['type'] == 'server':\n",
    "                servers_set.add(node)\n",
    "            elif G.nodes[node]['type'] == 'link':\n",
    "                links_set.add(node)\n",
    "    \n",
    "    for user in G_users:\n",
    "        users_set.add(user)\n",
    "\n",
    "    for app in containers_data:\n",
    "        applications_set.add(app)\n",
    "\n",
    "    # Add sets to the GAMSDatabase\n",
    "    db.add_set(\"servers\", servers_set)\n",
    "    db.add_set(\"links\", links_set)\n",
    "    db.add_set(\"users\", users_set)\n",
    "    db.add_set(\"applications\", applications_set)\n",
    "\n",
    "    # Write the GAMSDatabase to a GDX file\n",
    "    gdx_file = GdxFile(file=gdx_file_path, mode=\"wb\")\n",
    "    gdx_file.write_db(db)\n",
    "    gdx_file.close()\n",
    "\n",
    "# Example usage:\n",
    "G = nx.Graph()\n",
    "G.add_node(\"server1\", type=\"server\")\n",
    "G.add_node(\"server2\", type=\"server\")\n",
    "G.add_node(\"link1\", type=\"link\")\n",
    "G_users = [\"user1\", \"user2\"]\n",
    "containers_data = [\"app1\", \"app2\"]\n",
    "\n",
    "gdx_file_path = \"network_data.gdx\"\n",
    "create_gdx_file(G, G_users, containers_data, gdx_file_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything is testing after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "import json\n",
    "\n",
    "class Server:\n",
    "    \"\"\"Represents a server in the network.\"\"\"\n",
    "    def __init__(self, name, cpu='N/A', memory='N/A', storage='N/A'):\n",
    "        self.name = name\n",
    "        self.cpu = cpu\n",
    "        self.memory = memory\n",
    "        self.storage = storage\n",
    "\n",
    "class Link:\n",
    "    \"\"\"Represents a link between two nodes in the network.\"\"\"\n",
    "    def __init__(self, source, destination, capacity='N/A', cost='N/A', latency='N/A'):\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "        self.capacity = capacity\n",
    "        self.cost = cost\n",
    "        self.latency = latency\n",
    "\n",
    "class User:\n",
    "    \"\"\"Represents a user in the network.\"\"\"\n",
    "    def __init__(self, name, associated_ap='N/A', application='N/A', bandwidth='N/A', latency='N/A'):\n",
    "        self.name = name\n",
    "        self.associated_ap = associated_ap\n",
    "        self.application = application\n",
    "        self.bandwidth = bandwidth\n",
    "        self.latency = latency\n",
    "\n",
    "class CNF:\n",
    "    \"\"\"Represents a Cloud Native Function.\"\"\"\n",
    "    def __init__(self, name, storage, memory, cpu):\n",
    "        self.name = name\n",
    "        self.storage = storage\n",
    "        self.memory = memory\n",
    "        self.cpu = cpu\n",
    "\n",
    "class VNF(CNF):\n",
    "    \"\"\"Represents a Virtualized Network Function.\"\"\"\n",
    "    pass\n",
    "\n",
    "class Microservice(CNF):\n",
    "    \"\"\"Represents a microservice.\"\"\"\n",
    "    pass\n",
    "\n",
    "class User:\n",
    "    \"\"\"Represents a user in the network.\"\"\"\n",
    "    def __init__(self, name, associated_ap='N/A', application=None, bandwidth='N/A', latency='N/A'):\n",
    "        self.name = name\n",
    "        self.associated_ap = associated_ap\n",
    "        self.application = application\n",
    "        self.bandwidth = bandwidth\n",
    "        self.latency = latency\n",
    "\n",
    "class Application:\n",
    "    \"\"\"Represents an application running on the network.\"\"\"\n",
    "    def __init__(self, name, bandwidth, latency, device_density, sfc):\n",
    "        self.name = name\n",
    "        self.bandwidth = bandwidth\n",
    "        self.latency = latency\n",
    "        self.device_density = device_density\n",
    "        self.sfc = sfc\n",
    "        self.users = []  # List of associated users\n",
    "        self.vnfs = {}  # Dictionary to store VNFs\n",
    "        self.microservices = {}  # Dictionary to store microservices\n",
    "\n",
    "    def add_user(self, user):\n",
    "        \"\"\"Add a user associated with this application.\"\"\"\n",
    "        self.users.append(user)\n",
    "    \n",
    "    \n",
    "    def add_vnf(self, vnf):\n",
    "        \"\"\"Add a VNF to the application.\"\"\"\n",
    "        self.vnfs[vnf.name] = vnf\n",
    "\n",
    "    def add_microservice(self, microservice):\n",
    "        \"\"\"Add a microservice to the application.\"\"\"\n",
    "        self.microservices[microservice.name] = microservice\n",
    "\n",
    "\n",
    "def populate_users_and_associate_applications(G_users, applications):\n",
    "    \"\"\"Populate users from network data and associate them with applications.\"\"\"\n",
    "    users = {}\n",
    "    for user, attrs in G_users.nodes(data=True):\n",
    "        app_name = attrs.get('application', 'N/A')\n",
    "        if app_name in applications:\n",
    "            app = applications[app_name]\n",
    "        else:\n",
    "            app = None\n",
    "        user_obj = User(user, attrs.get('associated_ap', 'N/A'), app, attrs.get('bandwidth', 'N/A'), attrs.get('latency', 'N/A'))\n",
    "        if app:\n",
    "            app.add_user(user_obj)\n",
    "        users[user] = user_obj\n",
    "    return users\n",
    "\n",
    "# Replace the populate_applications function with the following code:\n",
    "\n",
    "def populate_applications(containers_data):\n",
    "    \"\"\"Populate applications from JSON data and associate them with CNFs.\"\"\"\n",
    "    applications = {}\n",
    "    for container in containers_data:\n",
    "        app_name = container.get('application', 'Unknown Application')\n",
    "        app = Application(app_name, container['bandwidth'], container['latency'], container['device_density'], container['sfc'])\n",
    "        for cnf in container.get('vnfs', []) + container.get('microservices', []):\n",
    "            cnf_type = \"Network Function\" if cnf in container.get('vnfs', []) else \"Analytics Function\"\n",
    "            cnf_obj = CNF(cnf['name'], cnf.get('storage', 'N/A'), cnf.get('memory', 'N/A'), cnf.get('cpu', 'N/A'))\n",
    "            if cnf_type == \"Network Function\":\n",
    "                app.add_vnf(cnf_obj)\n",
    "            else:\n",
    "                app.add_microservice(cnf_obj)\n",
    "        applications[app_name] = app\n",
    "    return applications\n",
    "\n",
    "\n",
    "\n",
    "def print_and_save_application_to_cnfs_mapping(applications, file_path):\n",
    "    \"\"\"Print and save the mapping of applications to CNFs.\"\"\"\n",
    "    with open(file_path, 'a') as f:\n",
    "        for app_name, app in applications.items():\n",
    "            cnfs_table = PrettyTable([\"Application\", \"Type\", \"Name\", \"CPU (Cores)\", \"Memory (GB)\", \"Storage (GB)\"])\n",
    "            for cnf_type, cnfs_dict in [(\"CNF\", app.vnfs), (\"Microservice\", app.microservices)]:\n",
    "                for cnf_name, cnf in cnfs_dict.items():\n",
    "                    cnfs_table.add_row([app_name, cnf_type, cnf.name, cnf.cpu, cnf.memory, cnf.storage])\n",
    "            print(cnfs_table)\n",
    "            f.write(str(cnfs_table) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def populate_servers(G):\n",
    "    \"\"\"Populate servers from network data.\"\"\"\n",
    "    servers = {}\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        server = Server(node, attrs.get('cpu', 'N/A'), attrs.get('memory', 'N/A'), attrs.get('storage', 'N/A'))\n",
    "        servers[node] = server\n",
    "    return servers\n",
    "\n",
    "def populate_links(G):\n",
    "    \"\"\"Populate links from network data.\"\"\"\n",
    "    links = []\n",
    "    for u, v, attrs in G.edges(data=True):\n",
    "        link = Link(u, v, attrs.get('capacity', 'N/A'), attrs.get('cost', 'N/A'), attrs.get('latency', 'N/A'))\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def populate_users(G_users):\n",
    "    \"\"\"Populate users from network data.\"\"\"\n",
    "    users = {}\n",
    "    for user, attrs in G_users.nodes(data=True):\n",
    "        user_obj = User(user, attrs.get('associated_ap', 'N/A'), attrs.get('application', 'N/A'), attrs.get('bandwidth', 'N/A'), attrs.get('latency', 'N/A'))\n",
    "        users[user] = user_obj\n",
    "    return users\n",
    "\n",
    "def populate_applications(containers_data):\n",
    "    \"\"\"Populate applications from JSON data.\"\"\"\n",
    "    applications = {}\n",
    "    for container in containers_data:\n",
    "        app_name = container.get('application', 'Unknown Application')\n",
    "        app = Application(app_name, container['bandwidth'], container['latency'], container['device_density'], container['sfc'])\n",
    "        for cnf in container.get('vnfs', []) + container.get('microservices', []):\n",
    "            cnf_type = \"Network Function\" if cnf in container.get('vnfs', []) else \"Analytics Function\"\n",
    "            cnf_obj = CNF(cnf['name'], cnf.get('storage', 'N/A'), cnf.get('memory', 'N/A'), cnf.get('cpu', 'N/A'))\n",
    "            if cnf_type == \"Network Function\":\n",
    "                app.add_vnf(cnf_obj)\n",
    "            else:\n",
    "                app.add_microservice(cnf_obj)\n",
    "        applications[app_name] = app\n",
    "    return applications\n",
    "\n",
    "def print_and_save_servers(servers, file_path):\n",
    "    \"\"\"Print and save server information.\"\"\"\n",
    "    with open(file_path, 'a') as f:\n",
    "        table = PrettyTable([\"Server\", \"CPU\", \"Memory\", \"Storage\"])\n",
    "        for server_name, server in servers.items():\n",
    "            table.add_row([server_name, server.cpu, server.memory, server.storage])\n",
    "        print(\"Servers and their capacities:\")\n",
    "        print(table)\n",
    "        f.write(\"Servers and their capacities:\\n\")\n",
    "        f.write(str(table) + \"\\n\")\n",
    "\n",
    "def print_and_save_network_graph_with_index(links, file_path):\n",
    "    \"\"\"Print and save network graph information.\"\"\"\n",
    "    with open(file_path, 'a') as f:\n",
    "        table = PrettyTable([\"#\", \"Link\", \"Capacity\", \"Cost\", \"Latency\"])\n",
    "        for index, link in enumerate(links, start=1):\n",
    "            table.add_row([index, f\"{link.source}-{link.destination}\", link.capacity, link.cost, link.latency])\n",
    "        print(\"Network Graph (Servers and Links):\")\n",
    "        print(table)\n",
    "        f.write(\"Network Graph (Servers and Links):\\n\")\n",
    "        f.write(str(table) + \"\\n\")\n",
    "\n",
    "def print_and_save_user_graph(users, file_path):\n",
    "    \"\"\"Print and save user graph information.\"\"\"\n",
    "    with open(file_path, 'a') as f:\n",
    "        table = PrettyTable([\"User\", \"Connected Node\", \"Application\", \"Bandwidth\", \"Latency\"])\n",
    "        for user_name, user in users.items():\n",
    "            table.add_row([user.name, user.associated_ap, user.application, user.bandwidth, user.latency])\n",
    "        print(\"User Graph:\")\n",
    "        print(table)\n",
    "        f.write(\"User Graph:\\n\")\n",
    "        f.write(str(table) + \"\\n\")\n",
    "\n",
    "def orchestrate_all(G, G_users, containers_data, app_to_users, user_to_node_map, file_path):\n",
    "    \"\"\"Orchestrate all data processing and printing/saving operations.\"\"\"\n",
    "    # Ensure the file is cleared before writing\n",
    "    open(file_path, 'w').close()\n",
    "    \n",
    "    # Populate data structures\n",
    "    servers = populate_servers(G)\n",
    "    links = populate_links(G)\n",
    "    users = populate_users(G_users)\n",
    "    applications = populate_applications(containers_data)\n",
    "    \n",
    "    # Print and save information\n",
    "    print_and_save_servers(servers, file_path)\n",
    "    print_and_save_network_graph_with_index(links, file_path)\n",
    "    print_and_save_user_graph(users, file_path)\n",
    "    print_and_save_application_to_cnfs_mapping(applications, file_path)\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"Devices\\containers.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Specify the path for the output file\n",
    "output_file_path = \"complete_network_mapping.txt\"\n",
    "\n",
    "# Call the orchestrating function\n",
    "orchestrate_all(G, G_users, data, app_to_users, user_to_node_map, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes.py\n",
    "\n",
    "class Server:\n",
    "    \"\"\"Represents a server in the network.\"\"\"\n",
    "    def __init__(self, name, cpu='N/A', memory='N/A', storage='N/A'):\n",
    "        self.name = name\n",
    "        self.cpu = cpu\n",
    "        self.memory = memory\n",
    "        self.storage = storage\n",
    "\n",
    "class Link:\n",
    "    \"\"\"Represents a link between two nodes in the network.\"\"\"\n",
    "    def __init__(self, source, destination, capacity='N/A', cost='N/A', latency='N/A'):\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "        self.capacity = capacity\n",
    "        self.cost = cost\n",
    "        self.latency = latency\n",
    "\n",
    "class User:\n",
    "    \"\"\"Represents a user in the network.\"\"\"\n",
    "    def __init__(self, name, associated_ap='N/A', application=None, bandwidth='N/A', latency='N/A'):\n",
    "        self.name = name\n",
    "        self.associated_ap = associated_ap\n",
    "        self.application = application\n",
    "        self.bandwidth = bandwidth\n",
    "        self.latency = latency\n",
    "        \n",
    "\n",
    "class CNF:\n",
    "    \"\"\"Represents a Cloud Native Function.\"\"\"\n",
    "    def __init__(self, name, storage, memory, cpu):\n",
    "        self.name = name\n",
    "        self.storage = storage\n",
    "        self.memory = memory\n",
    "        self.cpu = cpu\n",
    "\n",
    "class VNF(CNF):\n",
    "    \"\"\"Represents a Virtualized Network Function.\"\"\"\n",
    "    pass\n",
    "\n",
    "class Microservice(CNF):\n",
    "    \"\"\"Represents a microservice.\"\"\"\n",
    "    pass\n",
    "\n",
    "class Application:\n",
    "    \"\"\"Represents an application running on the network.\"\"\"\n",
    "    def __init__(self, name, bandwidth, latency, device_density, sfc):\n",
    "        self.name = name\n",
    "        self.bandwidth = bandwidth\n",
    "        self.latency = latency\n",
    "        self.device_density = device_density\n",
    "        self.sfc = sfc #order of cnfs\n",
    "        self.users = []  # List of associated users\n",
    "        self.cnfs = {}  # Dictionary to store both VNFs and Microservices\n",
    "\n",
    "App: Car - Slice1\n",
    "        Slice1: SFC1 + SFC2 + SFC3\n",
    "        SFC1: Firewall  + router\n",
    "\n",
    "\n",
    "\n",
    "    def add_user(self, user):\n",
    "        \"\"\"Add a user associated with this application.\"\"\"\n",
    "        self.users.append(user)\n",
    "    \n",
    "    def add_cnf(self, cnf, cnf_type):\n",
    "        \"\"\"Add a CNF (VNF or Microservice) to the application.\"\"\"\n",
    "        self.cnfs[cnf.name] = {'type': cnf_type, 'cnf': cnf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import json\n",
    "\n",
    "def add_cnf_to_application(app, cnf_data, cnf_type):\n",
    "    cnf = CNF(cnf_data.get('name', 'Unknown CNF'), cnf_data.get('storage', 'N/A'), cnf_data.get('memory', 'N/A'), cnf_data.get('cpu', 'N/A'))\n",
    "    app.add_cnf(cnf, cnf_type)\n",
    "\n",
    "def populate_applications(containers_data):\n",
    "    applications = {}\n",
    "    for container in containers_data:\n",
    "        app_name = container.get('application', 'Unknown Application')\n",
    "        app = Application(app_name, container.get('bandwidth', 'N/A'), container.get('latency', 'N/A'), container.get('device_density', 'N/A'), container.get('sfc', 'N/A'))\n",
    "        \n",
    "        for cnf_data in container.get('vnfs', []) + container.get('microservices', []):\n",
    "            cnf_type = 'vnf' if cnf_data in container.get('vnfs', []) else 'microservice'\n",
    "            add_cnf_to_application(app, cnf_data, cnf_type)\n",
    "        \n",
    "        applications[app_name] = app\n",
    "    return applications\n",
    "\n",
    "def populate_servers(G):\n",
    "    servers = {}\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        server = Server(node, attrs.get('cpu', 'N/A'), attrs.get('memory', 'N/A'), attrs.get('storage', 'N/A'))\n",
    "        servers[node] = server\n",
    "    return servers\n",
    "\n",
    "def populate_links(G):\n",
    "    links = []\n",
    "    for u, v, attrs in G.edges(data=True):\n",
    "        link = Link(u, v, attrs.get('capacity', 'N/A'), attrs.get('cost', 'N/A'), attrs.get('latency', 'N/A'))\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def populate_users(G_users):\n",
    "    users = {}\n",
    "    for user, attrs in G_users.nodes(data=True):\n",
    "        user_obj = User(user, attrs.get('associated_ap', 'N/A'), attrs.get('application', 'N/A'), attrs.get('bandwidth', 'N/A'), attrs.get('latency', 'N/A'))\n",
    "        users[user] = user_obj\n",
    "    return users\n",
    "\n",
    "def orchestrate_all(G, G_users, containers_data, output_file_path):\n",
    "    servers = populate_servers(G)\n",
    "    links = populate_links(G)\n",
    "    users = populate_users(G_users)\n",
    "    applications = populate_applications(containers_data)\n",
    "    \n",
    "    # Example function calls to print/save data (not fully implemented here)\n",
    "    print(\"Servers, links, users, and applications have been populated.\")\n",
    "\n",
    "# Example execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Load JSON data\n",
    "    with open(\"Devices\\containers.json\", \"r\") as json_file:\n",
    "        containers_data = json.load(json_file)\n",
    "\n",
    "    output_file_path = \"complete_network_mapping.txt\"\n",
    "    \n",
    "    # Mock data for G and G_users, assuming they're network graphs (e.g., from networkx library)\n",
    "    # G = some_network_graph\n",
    "    # G_users = some_user_graph\n",
    "    orchestrate_all(G, G_users, containers_data, output_file_path)\n",
    "    \n",
    "    print(\"Note: Actual network graphs (G, G_users) need to be defined for the complete execution.\")\n",
    "\n",
    "\n",
    "\n",
    "    from prettytable import PrettyTable\n",
    "\n",
    "def generate_sorted_cnf_user_summary(containers_data, app_to_users):\n",
    "    # Initialize a dictionary to hold CNF details and associated user count\n",
    "    cnf_details = {}\n",
    "\n",
    "    # Loop through each container (application) to gather CNF and user details\n",
    "    for container in containers_data['Containers']:\n",
    "        app_name = container['application']\n",
    "        users = app_to_users.get(app_name, [])  # Get users associated with this application\n",
    "        \n",
    "        # Process both Network Functions (vnfs) and Analytics Functions (microservices)\n",
    "        for cnf_type, cnf_list in [('Network Function', container.get('vnfs', [])), \n",
    "                                   ('Analytics Function', container.get('microservices', []))]:\n",
    "            for cnf in cnf_list:\n",
    "                cnf_name = cnf['name']\n",
    "                # Key to uniquely identify CNFs by name and type\n",
    "                cnf_key = (cnf_name, cnf_type)\n",
    "                \n",
    "                # Initialize or update the CNF entry in the dictionary\n",
    "                if cnf_key not in cnf_details:\n",
    "                    cnf_details[cnf_key] = {'applications': set(), 'users': set()}\n",
    "                cnf_details[cnf_key]['applications'].add(app_name)\n",
    "                cnf_details[cnf_key]['users'].update(users)\n",
    "                \n",
    "    # Sort CNF entries by the count of associated users in descending order\n",
    "    sorted_cnfs = sorted(cnf_details.items(), key=lambda item: len(item[1]['users']), reverse=True)\n",
    "    \n",
    "    # Create the summary table\n",
    "    summary_table = PrettyTable([\"CNF ID\", \"CNF Name\", \"CNF Type\", \"Applications\", \"Associated Users\", \"User Count\"])\n",
    "    cnf_id = 1\n",
    "    for (cnf_name, cnf_type), details in sorted_cnfs:\n",
    "        applications_str = ', '.join(sorted(details['applications']))\n",
    "        users_str = ', '.join(sorted([str(user.split('_')[-1]) for user in details['users']]))\n",
    "        user_count = len(details['users'])\n",
    "        summary_table.add_row([cnf_id, cnf_name, cnf_type, applications_str, users_str, user_count])\n",
    "        cnf_id += 1\n",
    "    \n",
    "    print(summary_table)\n",
    "\n",
    "# Assuming `containers_data` and `app_to_users` are correctly populated\n",
    "# Correctly call the function with your actual data\n",
    "# generate_sorted_cnf_user_summary(containers_data, app_to_users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# NetworkResource Class\n",
    "class NetworkResource:\n",
    "    def __init__(self, cpu=0, memory=0, storage=0):\n",
    "        self.cpu = cpu\n",
    "        self.memory = memory\n",
    "        self.storage = storage\n",
    "\n",
    "    def update(self, cpu_delta=0, memory_delta=0, storage_delta=0):\n",
    "        self.cpu += cpu_delta\n",
    "        self.memory += memory_delta\n",
    "        self.storage += storage_delta\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"CPU: {self.cpu} GHz, Memory: {self.memory} GB, Storage: {self.storage} GB\"\n",
    "\n",
    "# NetworkEntity Base Class\n",
    "class NetworkEntity:\n",
    "    def __init__(self, id, resources=None):\n",
    "        self.id = id\n",
    "        self.resources = resources or NetworkResource()\n",
    "\n",
    "# Server Class\n",
    "class Server(NetworkEntity):\n",
    "    def __init__(self, id, resources=None, status='active'):\n",
    "        super().__init__(id, resources)\n",
    "        self.status = status  # Server status (e.g., active, inactive)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_and_save_servers(G, file_path):\n",
    "        table = PrettyTable([\"ID\", \"Status\", \"CPU (GHz)\", \"Memory (GB)\", \"Storage (GB)\"])\n",
    "        for node, attrs in G.nodes(data=True):\n",
    "            if isinstance(attrs.get('entity'), Server):  # Ensure the node is a Server instance\n",
    "                server = attrs['entity']\n",
    "                table.add_row([server.id, server.status, f\"{server.resources.cpu} GHz\", f\"{server.resources.memory} GB\", f\"{server.resources.storage} GB\"])\n",
    "        print(table)\n",
    "        write_to_file(file_path, table.get_string())\n",
    "\n",
    "# Link Class\n",
    "class Link:\n",
    "    def __init__(self, source, destination, capacity, cost, latency):\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "        self.capacity = capacity\n",
    "        self.cost = cost\n",
    "        self.latency = latency\n",
    "\n",
    "# User Class\n",
    "class User(NetworkEntity):\n",
    "    def __init__(self, id, associated_ap='N/A', application=None, bandwidth='N/A', latency='N/A', resources=None):\n",
    "        super().__init__(id, resources)\n",
    "        self.associated_ap = associated_ap\n",
    "        self.application = application\n",
    "        self.bandwidth = bandwidth\n",
    "        self.latency = latency\n",
    "\n",
    "# ContainerNetworkFunction (CNF) Class\n",
    "class ContainerNetworkFunction(NetworkEntity):\n",
    "    def __init__(self, id, resources, cnf_type='NF'):\n",
    "        super().__init__(id, resources)\n",
    "        self.cnf_type = cnf_type\n",
    "\n",
    "# Application Class\n",
    "class Application(NetworkEntity):\n",
    "    def __init__(self, id, required_resources, sfc_id=None):\n",
    "        super().__init__(id, required_resources)\n",
    "        self.sfc_id = sfc_id\n",
    "        self.cnfs = []\n",
    "\n",
    "    def add_cnf(self, cnf):\n",
    "        self.cnfs.append(cnf)\n",
    "\n",
    "# Additional classes and methods can be added as needed.\n",
    "\n",
    "# Helper function for writing to a file\n",
    "def write_to_file(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(data + \"\\n\")\n",
    "\n",
    "# Orchestration function to manage network operations\n",
    "def orchestrate_all(G, file_path):\n",
    "    # Ensure the file is cleared before writing\n",
    "    open(file_path, 'w').close()\n",
    "    \n",
    "    # Print and save server information\n",
    "    Server.print_and_save_servers(G, file_path)\n",
    "    # Additional logic for printing and saving links, users, CNFs, and applications\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    G = nx.Graph()  # Example network graph initialization\n",
    "    file_path = \"network_summary.txt\"\n",
    "    # Populate the graph G with network entities\n",
    "    \n",
    "    # Example of adding a server to the graph\n",
    "    server_resources = NetworkResource(16, 64, 1)\n",
    "    server1 = Server(\"Server1\", server_resources)\n",
    "    G.add_node(server1.id, entity=server1)\n",
    "    \n",
    "    # Call the orchestration function\n",
    "    orchestrate_all(G, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW GAMS APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & print variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "def write_to_file(file_path, data):\n",
    "    with open(file_path, 'a') as file:  # Append mode\n",
    "        file.write(data + \"\\n\")\n",
    "\n",
    "\n",
    "def print_and_save_servers(G, file_path):\n",
    "    table = PrettyTable([\"Server\", \"CPU\", \"Memory\", \"Storage\"])\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        cpu = f\"{attrs.get('cpu', 'N/A')} GHz\"\n",
    "        memory = f\"{attrs.get('memory', 'N/A')} GB\"\n",
    "        storage = f\"{attrs.get('storage', 'N/A')} TB\"\n",
    "        table.add_row([node, cpu, memory, storage])\n",
    "    print(\"Servers and their capacities:\\n\" + table.get_string())\n",
    "    write_to_file(file_path, \"Servers and their capacities:\\n\" + table.get_string())\n",
    "\n",
    "def print_and_save_network_graph_with_index(G, file_path):\n",
    "    table = PrettyTable([\"#\", \"Link\", \"Capacity\", \"Cost\", \"Latency\"])\n",
    "    for index, (u, v, attrs) in enumerate(G.edges(data=True), start=1):\n",
    "        table.add_row([index, f\"{u}-{v}\", attrs.get('capacity', 'N/A'), attrs.get('cost', 'N/A'), attrs.get('latency', 'N/A')])\n",
    "    print(\"\\nNetwork Graph (Servers and Links):\\n\" + table.get_string())\n",
    "    write_to_file(file_path, \"\\nNetwork Graph (Servers and Links):\\n\" + table.get_string())\n",
    "\n",
    "def print_and_save_user_graph(G_users, file_path):\n",
    "    table = PrettyTable([\"User\", \"Connected Node\", \"Application\", \"Bandwidth\", \"Latency\"])\n",
    "    for user, attrs in G_users.nodes(data=True):\n",
    "        table.add_row([user, attrs.get('associated_ap', 'N/A'), attrs.get('application', 'N/A'), attrs.get('bandwidth', 'N/A'), attrs.get('latency', 'N/A')])\n",
    "    print(\"\\nUser Graph:\\n\" + table.get_string())\n",
    "    write_to_file(file_path, \"\\nUser Graph:\\n\" + table.get_string())\n",
    "\n",
    "def print_and_save_users_and_apps(app_to_users, user_to_node_map, file_path):\n",
    "    table = PrettyTable([\"Application\", \"User\", \"Connected Node\"])\n",
    "    for app, users in app_to_users.items():\n",
    "        for user in users:\n",
    "            table.add_row([app, user, user_to_node_map.get(user, 'N/A')])\n",
    "    print(\"\\nUsers and Applications:\\n\" + table.get_string())\n",
    "    write_to_file(file_path, \"\\nUsers and Applications:\\n\" + table.get_string())\n",
    "\n",
    "def print_and_save_application_to_cnfs_mapping(containers_data, file_path):\n",
    "    for application in containers_data.get('Containers', []):\n",
    "        app_name = application.get('application', 'Unknown Application')\n",
    "        cnfs_table = PrettyTable([\"Application\", \"Type\", \"Name\", \"CPU (Cores)\", \"Memory (GB)\", \"Storage (GB)\"])\n",
    "        for cnf in application.get('vnfs', []):  # Network Functions\n",
    "            cnfs_table.add_row([app_name, \"Network Function\", cnf['name'], cnf['cpu'], cnf['memory'], cnf['storage']])\n",
    "        for analytics_cnf in application.get('microservices', []):  # Analytics Functions\n",
    "            cnfs_table.add_row([app_name, \"Analytics Function\", analytics_cnf['name'], analytics_cnf['cpu'], analytics_cnf['memory'], analytics_cnf['storage']])\n",
    "        print(cnfs_table.get_string())\n",
    "        write_to_file(file_path, cnfs_table.get_string() + \"\\n\")\n",
    "\n",
    "# Define a function to orchestrate all printing and saving operations\n",
    "def orchestrate_all(file_path):\n",
    "    # Ensure the file is cleared before writing\n",
    "    open(file_path, 'w').close()\n",
    "    \n",
    "    # Call each function with the file path to append data\n",
    "    print_and_save_servers(G, file_path)\n",
    "    print_and_save_network_graph_with_index(G, file_path)\n",
    "    print_and_save_user_graph(G_users, file_path)\n",
    "    print_and_save_users_and_apps(app_to_users, user_to_node_map, file_path)\n",
    "    print_and_save_application_to_cnfs_mapping(containers_data, file_path)\n",
    "\n",
    "# Specify the path for the output file\n",
    "#output_file_path = \"complete_network_mapping.txt\"\n",
    "\n",
    "# Call the orchestrating function\n",
    "#orchestrate_all(output_file_path)\n",
    "\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def generate_comprehensive_table(containers_data, app_to_users):\n",
    "    table = PrettyTable([\"App ID\", \"Application\", \"CNF Type\", \"CNF Name\", \"CPU (Cores)\", \"Memory (GB)\", \"Storage (GB)\", \"Associated Users\"])\n",
    "    \n",
    "    app_id = 1  # Initialize application ID counter\n",
    "    for container in containers_data['Containers']:\n",
    "        app_name = container['application']\n",
    "        # Gather all users associated with this application\n",
    "        users = ', '.join(app_to_users.get(app_name, []))\n",
    "        \n",
    "        # Process each CNF (Network Function and Analytics Function)\n",
    "        for cnf in container.get('vnfs', []) + container.get('microservices', []):\n",
    "            cnf_type = \"Network Function\" if cnf in container.get('vnfs', []) else \"Analytics Function\"\n",
    "            table.add_row([app_id, app_name, cnf_type, cnf['name'], cnf['cpu'], cnf['memory'], cnf['storage'], users])\n",
    "        \n",
    "        app_id += 1  # Increment application ID for the next application\n",
    "    \n",
    "    print(table)\n",
    "    # Save the table to a file\n",
    "    with open(\"comprehensive_network_mapping.txt\", \"w\") as f:\n",
    "        f.write(table.get_string())\n",
    "\n",
    "# Assuming `containers_data` and `app_to_users` are defined\n",
    "#generate_comprehensive_table(containers_data, app_to_users)\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def generate_sorted_cnf_user_summary(containers_data, app_to_users):\n",
    "    # Initialize dictionaries to hold CNF details and associated user count for each type\n",
    "    network_functions = {}\n",
    "    analytics_functions = {}\n",
    "\n",
    "    # Loop through each container (application) to gather CNF and user details\n",
    "    for container in containers_data:\n",
    "        app_name = container['application']\n",
    "        users = app_to_users.get(app_name, [])  # Get users associated with this application\n",
    "        \n",
    "        # Process both Network Functions (vnfs) and Analytics Functions (microservices)\n",
    "        for cnf_type, cnf_list in [('Network Function', container.get('vnfs', [])), \n",
    "                                   ('Analytics Function', container.get('microservices', []))]:\n",
    "            for cnf in cnf_list:\n",
    "                cnf_name = cnf['name']\n",
    "                # Determine which dictionary to use based on the CNF type\n",
    "                if cnf_type == 'Network Function':\n",
    "                    cnf_dict = network_functions\n",
    "                else:\n",
    "                    cnf_dict = analytics_functions\n",
    "                \n",
    "                # Key to uniquely identify CNFs by name and type\n",
    "                cnf_key = (cnf_name, cnf_type)\n",
    "                \n",
    "                # Initialize or update the CNF entry in the dictionary\n",
    "                if cnf_key not in cnf_dict:\n",
    "                    cnf_dict[cnf_key] = {'applications': set(), 'users': set()}\n",
    "                cnf_dict[cnf_key]['applications'].add(app_name)\n",
    "                cnf_dict[cnf_key]['users'].update(users)\n",
    "    \n",
    "    # Sort the network functions and analytics functions separately\n",
    "    sorted_network_functions = sorted(network_functions.items(), key=lambda item: len(item[1]['users']), reverse=True)\n",
    "    sorted_analytics_functions = sorted(analytics_functions.items(), key=lambda item: len(item[1]['users']), reverse=True)\n",
    "    \n",
    "    # Merge the sorted lists into a single list\n",
    "    sorted_cnfs = sorted_network_functions + sorted_analytics_functions\n",
    "    \n",
    "    # Create the summary table\n",
    "    summary_table = PrettyTable([\"CNF ID\", \"CNF Name\", \"CNF Type\", \"Applications\", \"Associated Users\", \"User Count\"])\n",
    "    cnf_id = 1\n",
    "    for (cnf_name, cnf_type), details in sorted_cnfs:\n",
    "        applications_str = ', '.join(sorted(details['applications']))\n",
    "        users_str = ', '.join(sorted([str(user.split('_')[-1]) for user in details['users']]))\n",
    "        user_count = len(details['users'])\n",
    "        summary_table.add_row([cnf_id, cnf_name, cnf_type, applications_str, users_str, user_count])\n",
    "        cnf_id += 1\n",
    "    \n",
    "    return summary_table\n",
    "\n",
    "def write_summary_to_file(summary_table, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(str(summary_table))\n",
    "\n",
    "# Assuming `containers_data` and `app_to_users` are correctly populated\n",
    "# Correctly call the function with your actual data\n",
    "summary_table = generate_sorted_cnf_user_summary(containers_data, app_to_users)\n",
    "write_summary_to_file(summary_table, \"sorted_cnf_user_summary.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
